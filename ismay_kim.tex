\documentclass[]{tufte-book}

% ams
\usepackage{amssymb,amsmath}

\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \makeatletter
  \@ifpackageloaded{fontspec}{}{\usepackage{fontspec}}
  \makeatother
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
  \makeatletter
  \@ifpackageloaded{soul}{
     \renewcommand\allcapsspacing[1]{{\addfontfeature{LetterSpace=15}#1}}
     \renewcommand\smallcapsspacing[1]{{\addfontfeature{LetterSpace=10}#1}}
   }{}
  \makeatother
\fi

% graphix
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}

% booktabs
\usepackage{booktabs}

% url
\usepackage{url}

% hyperref
\usepackage{hyperref}

% units.
\usepackage{units}


\setcounter{secnumdepth}{2}

% citations
\usepackage{natbib}
\bibliographystyle{apalike}

% pandoc syntax highlighting
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}

% longtable
\usepackage{longtable,booktabs}

% multiplecol
\usepackage{multicol}

% strikeout
\usepackage[normalem]{ulem}

% morefloats
\usepackage{morefloats}


% tightlist macro required by pandoc >= 1.14
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% title / author / date
\title{\Large{A MODERN DIVE into Data with R}}
\author{Chester Ismay and Albert Y. Kim}
\date{2016-08-26}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

%% Need to clean up
\newenvironment{rmdblock}[1]
  {\begin{shaded*}
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
  %    {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \item
  }
  {
  \end{itemize}
  \end{shaded*}
  }
%% Probably can be omitted
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{learncheck}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{review}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

% To tweak tufte layout
\geometry{
  left=0.8in, % left margin
  textwidth=35pc, % main text block
  marginparsep=1pc, % gutter between main text block and margin notes
  marginparwidth=8pc % width of margin notes
}

\begin{document}

\maketitle



{
\setcounter{tocdepth}{1}
\tableofcontents
}

\chapter{Prerequisites}\label{prereqs}

This book was written using the \textbf{bookdown} R package from Yihui
Xie. In order to follow along and run the code in this book on your own,
you'll need to have access to R and RStudio. You can find more
information on both of these with a simple Google search for ``R'' and
for ``RStudio''. An introduction to using R, RStudio, and R Markdown is
also available in a free book
\href{http://ismayc.github.io/rbasics-book}{here} \citep{usedtor2016}.
It is recommended that you refer back to this book frequently as it has
GIF screen recordings that you can follow along with as you learn.

We will keep a running list of R packages you will need to have
installed to complete the analysis as well here in the
\texttt{needed\_pkgs} character vector. You can check if you have all of
the needed packages installed by running all of the lines below. The
last lines including the \texttt{if} will install them as needed (i.e.,
download their needed files from the internet to your hard drive).

You can run the \texttt{library} function on them to load them into your
current analysis. Prior to each analysis where a package is needed, you
will see the corresponding \texttt{library} function in the text.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{needed_pkgs <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"nycflights13"}\NormalTok{, }\StringTok{"dplyr"}\NormalTok{, }\StringTok{"ggplot2"}\NormalTok{, }\StringTok{"knitr"}\NormalTok{, }
  \StringTok{"devtools"}\NormalTok{, }\StringTok{"ggplot2movies"}\NormalTok{, }\StringTok{"dygraphs"}\NormalTok{, }\StringTok{"rmarkdown"}\NormalTok{)}

\NormalTok{new.pkgs <-}\StringTok{ }\NormalTok{needed_pkgs[!(needed_pkgs %in%}\StringTok{ }\KeywordTok{installed.packages}\NormalTok{())]}

\NormalTok{if(}\KeywordTok{length}\NormalTok{(new.pkgs)) \{}
  \KeywordTok{install.packages}\NormalTok{(new.pkgs, }\DataTypeTok{repos =} \StringTok{"http://cran.rstudio.com"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\textbf{Book was last updated:}

\begin{verbatim}
## [1] "By Chester on Friday, August 26, 2016 16:45:08 PDT"
\end{verbatim}

\section{Colophon}\label{colophon}

The source of the book is available
\href{https://github.com/ModernStats/bookdown}{here} and was built with
versions of R packages given here. This may not be of importance for
initial readers of this book, but you can reproduce a duplicate of this
book by installing these versions of the packages.

\begin{longtable}{lllll}
\toprule
package & * & version & date & source\\
\midrule
assertthat &  & 0.1 & 2013-12-06 & CRAN (R 3.3.0)\\
base64enc &  & 0.1-3 & 2015-07-28 & CRAN (R 3.3.0)\\
BH &  & 1.60.0-2 & 2016-05-07 & CRAN (R 3.3.0)\\
bitops &  & 1.0-6 & 2013-08-17 & CRAN (R 3.3.0)\\
caTools &  & 1.17.1 & 2014-09-10 & CRAN (R 3.3.0)\\
\addlinespace
colorspace &  & 1.2-6 & 2015-03-11 & CRAN (R 3.3.0)\\
curl &  & 1.2 & 2016-08-13 & CRAN (R 3.3.0)\\
DBI &  & 0.5 & 2016-08-11 & CRAN (R 3.3.0)\\
devtools &  & 1.12.0 & 2016-06-24 & CRAN (R 3.3.0)\\
dichromat &  & 2.0-0 & 2013-01-24 & CRAN (R 3.3.0)\\
\addlinespace
digest &  & 0.6.10 & 2016-08-02 & CRAN (R 3.3.0)\\
dplyr & * & 0.5.0 & 2016-06-24 & CRAN (R 3.3.0)\\
dygraphs & * & 1.1.1-1 & 2016-08-06 & CRAN (R 3.3.0)\\
evaluate &  & 0.9 & 2016-04-29 & CRAN (R 3.3.0)\\
formatR &  & 1.4 & 2016-05-09 & CRAN (R 3.3.0)\\
\addlinespace
ggplot2 & * & 2.1.0 & 2016-03-01 & CRAN (R 3.3.0)\\
ggplot2movies &  & 0.0.1 & 2015-08-25 & CRAN (R 3.3.0)\\
git2r &  & 0.15.0 & 2016-05-11 & CRAN (R 3.3.0)\\
gtable &  & 0.2.0 & 2016-02-26 & CRAN (R 3.3.0)\\
highr &  & 0.6 & 2016-05-09 & CRAN (R 3.3.0)\\
\addlinespace
htmltools &  & 0.3.5 & 2016-03-21 & CRAN (R 3.3.0)\\
htmlwidgets &  & 0.7 & 2016-08-02 & CRAN (R 3.3.0)\\
httr &  & 1.2.1 & 2016-07-03 & CRAN (R 3.3.0)\\
jsonlite &  & 1.0 & 2016-07-01 & CRAN (R 3.3.0)\\
knitr & * & 1.14 & 2016-08-13 & CRAN (R 3.3.0)\\
\addlinespace
labeling &  & 0.3 & 2014-08-23 & CRAN (R 3.3.0)\\
lattice &  & 0.20-33 & 2015-07-14 & CRAN (R 3.3.1)\\
lazyeval &  & 0.2.0 & 2016-06-12 & CRAN (R 3.3.0)\\
magrittr &  & 1.5 & 2014-11-22 & CRAN (R 3.3.0)\\
markdown &  & 0.7.7 & 2015-04-22 & CRAN (R 3.3.0)\\
\addlinespace
MASS &  & 7.3-45 & 2016-04-21 & CRAN (R 3.3.1)\\
memoise &  & 1.0.0 & 2016-01-29 & CRAN (R 3.3.0)\\
mime &  & 0.5 & 2016-07-07 & CRAN (R 3.3.0)\\
munsell &  & 0.4.3 & 2016-02-13 & CRAN (R 3.3.0)\\
nycflights13 & * & 0.2.0 & 2016-04-30 & CRAN (R 3.3.0)\\
\addlinespace
openssl &  & 0.9.4 & 2016-05-25 & CRAN (R 3.3.0)\\
plyr &  & 1.8.4 & 2016-06-08 & CRAN (R 3.3.0)\\
R6 &  & 2.1.3 & 2016-08-19 & CRAN (R 3.3.0)\\
RColorBrewer &  & 1.1-2 & 2014-12-07 & CRAN (R 3.3.0)\\
Rcpp &  & 0.12.6 & 2016-07-19 & CRAN (R 3.3.0)\\
\addlinespace
reshape2 &  & 1.4.1 & 2014-12-06 & CRAN (R 3.3.0)\\
rmarkdown &  & 1.0 & 2016-07-08 & CRAN (R 3.3.0)\\
rstudioapi &  & 0.6 & 2016-06-27 & CRAN (R 3.3.0)\\
scales &  & 0.4.0 & 2016-02-26 & CRAN (R 3.3.0)\\
stringi &  & 1.1.1 & 2016-05-27 & CRAN (R 3.3.0)\\
\addlinespace
stringr &  & 1.1.0 & 2016-08-19 & CRAN (R 3.3.0)\\
tibble &  & 1.1 & 2016-07-04 & CRAN (R 3.3.0)\\
whisker &  & 0.3-2 & 2013-04-28 & CRAN (R 3.3.0)\\
withr &  & 1.0.2 & 2016-06-20 & CRAN (R 3.3.0)\\
xts &  & 0.9-7 & 2014-01-02 & CRAN (R 3.3.0)\\
\addlinespace
yaml &  & 2.1.13 & 2014-06-12 & CRAN (R 3.3.0)\\
zoo &  & 1.7-13 & 2016-05-03 & CRAN (R 3.3.0)\\
\bottomrule
\end{longtable}

\chapter{Introduction}\label{intro}

\section{Preamble}\label{preamble}

This book is inspired by three books:

\begin{itemize}
\tightlist
\item
  ``Mathematical Statistics with Resampling and R'' \citep{hester2011},
\item
  ``Intro Stat with Randomization and Simulation'' \citep{isrs2014}, and
\item
  ``R for Data Science'' \citep{r4ds2016}.
\end{itemize}

These books provide excellent resources on how to use resampling to
build statistical concepts like normal distributions using computers
instead of focusing on memorization of formulas. The last two books also
provide a path towards free alternatives to the traditionally expensive
introductory statistics textbook. When looking over the vast number of
introductory statistics textbooks we found that there wasn't one that
incorporated many of the new R packages directly into the text.
Additionally, there wasn't an open-source, free textbook available that
showed new learners all of the following

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  how to use R to explore and visualize data
\item
  how to use randomization and simulation to build inferential ideas
\item
  how to effectively create stories using these ideas to convey
  information to a lay audience.
\end{enumerate}

We will introduce sometimes difficult statistics concepts through the
medium of data visualization. In today's world, we are bombarded with
graphics that attempt to convey ideas. We will explore what makes a good
graphic and what the standard ways are to convey relationships with
data. You'll also see the use of visualization to introduce concepts
like mean, median, standard deviation, distributions, etc. In general,
we'll use visualization as a way of building almost all of the ideas in
this book.

Additionally, this book will focus on the triad of computational
thinking, data thinking, and inferential thinking. We'll see throughout
the book how these three modes of thinking can build effective ways to
work with, describe, and convey statistical knowledge. In order to do
so, you'll see the importance of literate programming to develop
literate data science. In other words, you'll see how to write code and
descriptions that are useful not just for a computer to execute but also
for readers to understand exactly what a statistical analysis is doing
and how it works. Hal Abelson coined the phrase that we will follow
throughout this book:

\begin{quote}
``Programs must be written for people to read, and only incidentally for
machines to execute.''
\end{quote}

\section{Two driving data sources}\label{two-driving-data-sources}

Instead of hopping from one data set to the next, we've decided to focus
throughout the book on two different data sources: flights leaving New
York City in 2013 and IMDB (Internet Movie DataBase) ratings on movies.
By focusing on just two large data sources, it is our hope that you'll
be able to see how each of the chapters is interconnected. You'll see
how the data being tidy leads into data visualization and manipulation
and how those concepts tie into inference and regression.

\section{Data/science pipeline}\label{datascience-pipeline}

You may think of statistics as just being a bunch of numbers. We
commonly hear the phrase ``statistician'' when listening to broadcasts
of sporting events. Statistics (in particular, data analysis), in
addition to describing numbers like with baseball batting averages,
plays a vital role in all of the sciences. You'll commonly hear the
phrase ``statistically significant'' thrown around in the media. You'll
see things that say ``Science now shows that chocolate is good for
you.'' Underpinning these claims is data analysis. By the end of this
book, you'll be able to better understand whether these claims should be
trusted or whether we should be weary. Inside data analysis are many
sub-fields that we will discuss throughout this book (not necessarily in
this order):

\begin{itemize}
\tightlist
\item
  data collection
\item
  data manipulation
\item
  data visualization
\item
  data modeling
\item
  inference
\item
  interpretation of results
\item
  data storytelling
\end{itemize}

This can be summarized in a graphic that is commonly used by Hadley
Wickham:

\begin{center}\includegraphics[width=13.01in]{images/tidy1} \end{center}

We will begin with a discussion on what is meant by tidy data and then
dig into the gray \textbf{Understand} portion of the cycle and conclude
by talking about interpretting and discussing the results of our models
via \textbf{Communication}. These steps are vital to any statistical
analysis. But why should you care about statistics? ``Why did they make
me take this class?''

There's a reason so many fields require a statistics course. Scientific
knowledge grows through an understanding of statistical significance and
data analysis. You needn't be intimidated by statistics. It's not the
beast that it used to be and paired with computation you'll see how
reproducible research in the sciences particularly increases scientific
knowledge.

\section{Reproducibility}\label{reproducibility}

\begin{quote}
``The most important tool is the \emph{mindset}, when starting, that the
end product will be reproducible.'' -- Keith Baggerly
\end{quote}

Another large goal of this book is to help readers understand the
importance of reproducible analyses. The hope is to get readers into the
habit of making their analyses reproducible from the very beginning.
This means we'll be trying to help you build new habits. This will take
practice and be difficult at times. You'll see just why it is so
important for you to keep track of your code and well-document it to
help yourself later and any potential collaborators as well.

Copying and pasting is not the way that efficient and effective
scientific research is conducted. It's much more important for time to
be spent on data collection and data analysis and not on copying and
pasting plots back and forth across a variety of programs.

In a traditional analyses if an error was made with the original data,
we'd need to step through the entire process again: recreate the plots
and copy and paste all of the new plots and our statistical analsis into
your document. This is error prone and a frustrating use of time. We'll
see how to use R Markdown to get away from this tedious activity so that
we can spend more time doing science.

\begin{quote}
``We are talking about \emph{computational} reproducibility.'' - Yihui
Xie
\end{quote}

Reproducibility means a lot of things in terms of different scientific
fields. Are experiments conducted in a way that another researcher could
follow the steps and get similar results? In this book, we will focus on
what is known as \textbf{computational reproducibility}. This refers to
being able to pass all of one's data analysis and conclusions to someone
else and have them get exactly the same results on their machine. This
allows for time to be spent doing actual science and interpretting of
results and assumptions instead of the more error prone way of starting
from scratch or follow a list of steps that may be different from
machine to machine.

\section{Who is this book for?}\label{who-is-this-book-for}

This book is targetted at students taking a traditional intro stats
class in a small college environment using RStudio and preferably
RStudio Server. We assume no prerequisites: no calculus and no prior
programming experience. This is intended to be a gentle and nice
introduction to the practice of statistics in terms of how data
scientists, statisticians, and other scientists analyze data and write
stories about data. We have intentionally avoided the use of throwing
formulas at you and instead have focused on developing statistical
concepts via data visualization and statistical computing. We hope this
is a more intuitive experience than the way statistics has traditionally
been taught in the past (and how it is commonly perceived from the
outside). We additionally hope that you see the value of reproducible
research via R as you continue in your studies. We understand that there
will initially be growing pains in learning to program but we are here
to help you and you should know that there is a huge community of R
users that are always happy to help newbies along.

Now let's get into learning about how to create good stories about and
with data!

\part{Data Exploration}\label{part-data-exploration}


\chapter{Tidy data}\label{tidy}

In this chapter, we'll discuss the importance of tidy data. You may
think that this means just having your data in a spreadsheet, but you'll
see that it is actually more specific than that. Data actually comes to
us in a variety of formats from pictures to text and to just numbers.
We'll focus on datasets that can be stored in a spreadsheet throughout
this book as that is the most common way data is collected in the
sciences.

Having tidy data will allow us to more easily create data visualizations
as we will see in Chapter \ref{viz}. It will also help us with
manipulating data in Chapter \ref{manip} and in all subsequent chapters
when we discuss statistical inference. You may not necessarily
understand the importance for \textbf{tidy data} but it will become more
and more apparent as we proceed through the book.

\section{What is tidy data?}\label{what-is-tidy-data}

You have surely heard the word ``tidy'' in your life:

\begin{itemize}
\tightlist
\item
  ``Tidy up your room!''
\item
  ``Please write your homework in a tidy way so that it is easier to
  grade and to provide feedback.''
\item
  Marie Kondo's best-selling book
  \href{https://www.amazon.com/Life-Changing-Magic-Tidying-Decluttering-Organizing/dp/1607747308/ref=sr_1_1?ie=UTF8\&qid=1469400636\&sr=8-1\&keywords=tidying+up}{\emph{The
  Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering
  and Organizing}}
\item
  ``I am not by any stretch of the imagination a tidy person, and the
  piles of unread books on the coffee table and by my bed have a
  plaintive, pleading quality to me - `Read me, please!'\,'' - Linda
  Grant
\end{itemize}

So what does it mean for your data to be \textbf{tidy}? Put simply: it
means that your data is organized. But it's more than just that. It
means that your data follows the same standard format making it easy for
others to find elements of your data, to manipulate and transform your
data, and for our purposes continuing with the common theme: it makes it
easier to visualize your data and the relationships between different
variables in your data.

We will follow Hadley Wickham's definition of \textbf{tidy data} here
\citep{tidy}:

\begin{quote}
A dataset is a collection of values, usually either numbers (if
quantitative) or strings (if qualitative). Values are organised in two
ways. Every value belongs to a variable and an observation. A variable
contains all values that measure the same underlying attribute (like
height, temperature, duration) across units. An observation contains all
values measured on the same unit (like a person, or a day, or a race)
across attributes.
\end{quote}

\begin{quote}
Tidy data is a standard way of mapping the meaning of a dataset to its
structure. A dataset is messy or tidy depending on how rows, columns and
tables are matched up with observations, variables and types. In
\textbf{tidy data}:
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable forms a column.
\item
  Each observation forms a row.
\item
  Each type of observational unit forms a table.
\end{enumerate}
\end{quote}

\begin{figure}

{\centering \includegraphics[width=26.67in]{images/tidy-1} 

}

\caption[Tidy data graphic from http://r4ds.had.co.nz/tidy-data.html]{Tidy data graphic from http://r4ds.had.co.nz/tidy-data.html}\label{fig:tidyfig}
\end{figure}

Reading over this definition, you can begin to think about datasets that
won't follow this nice format.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC3.1)} Give an example dataset that doesn't follow this
format.

\begin{itemize}
\tightlist
\item
  What features of this dataset might make it difficult to visualize?\\
\item
  How could the dataset be tweaked to make it \textbf{tidy}?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{\texorpdfstring{The \texttt{nycflights13}
datasets}{The nycflights13 datasets}}\label{the-nycflights13-datasets}

We likely have all flown on airplanes or know someone that has. Air
travel has become an ever-present aspect of our daily lives. If you live
in or are visiting a relatively large city and you walk around that
city's airport, you see gates showing flight information from many
different airlines. And you will frequently see that some flights are
delayed because of a variety of conditions. Are there ways that we can
avoid having to deal with these flight delays?

We'd all like to arrive at our destinations on time whenever possible.
(Unless you secretly love hanging out at airports. If you are one of
these people, pretend for the moment that you are very much anticipating
being at your final destination.) Hadley Wickham (herein just referred
to as ``Hadley'') created multiple datasets containing information about
departing flights from the New York City area in 2013
\citep{R-nycflights13}. We will begin by loading in one of these
datasets, the \texttt{flights} dataset, and getting an idea of its
structure:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nycflights13)}
\KeywordTok{data}\NormalTok{(flights)}
\end{Highlighting}
\end{Shaded}

The \texttt{library} function here loads the R package
\texttt{nycflights13} into the current R environment in which you are
working. (Note that you'll get an error if you try to load this package
in and it hasn't been installed. Check Chapter \ref{intro} to make sure
the package has been downloaded.) The next line of code
\texttt{data(flights)} loads in the \texttt{flights} dataset that is
stored in the \texttt{nycflights13} package.

This dataset and most others presented in this book will be in the
\texttt{data.frame} format in R. Dataframes are ways to look at
collections of variables that are tightly coupled together. Frequently,
the best way to get a feel for a dataframe is to use the \texttt{View}
function in RStudio. This command will be given throughout the book as a
reminder, but the actual output will be hidden.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{View}\NormalTok{(flights)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC3.4)} What does any \emph{ONE} row in this dataset refer to?

\begin{itemize}
\tightlist
\item
  A. Data on an airline
\item
  B. Data on a flight
\item
  C. Data on an airport
\item
  D. Data on multiple flights
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

By running \texttt{View(flights)}, we see the different
\textbf{variables} listed in the columns and we see that there are
different types of variables. Some of the variables like
\texttt{distance}, \texttt{day}, and \texttt{arr\_delay} are what we
will call \textbf{quantitative} variables. These variables vary in a
numerical way. Other variables here are \textbf{categorical}.

Note that if you look in the leftmost column of the
\texttt{View(flights)} output, you will see a column of numbers. These
are the row numbers of the dataset. If you glance across a row with the
same number, say row 5, you can get an idea of what each row correspond
to. In other words, this will allow you to identify what object is being
referred to in a given row. This is often called the
\textbf{observational unit}. The \textbf{observational unit} in this
example is an individual flight departing New York City in 2013.

\textbf{Note}: Frequently the first thing you should do when given a
dataset is to

\begin{itemize}
\tightlist
\item
  identify the observation unit,
\item
  specify the variables, and
\item
  give the types of variables you are presented with.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'tbl_df', 'tbl' and 'data.frame':    336776 obs. of  19 variables:
##  $ year          : int  2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...
##  $ month         : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ day           : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ dep_time      : int  517 533 542 544 554 554 555 557 557 558 ...
##  $ sched_dep_time: int  515 529 540 545 600 558 600 600 600 600 ...
##  $ dep_delay     : num  2 4 2 -1 -6 -4 -5 -3 -3 -2 ...
##  $ arr_time      : int  830 850 923 1004 812 740 913 709 838 753 ...
##  $ sched_arr_time: int  819 830 850 1022 837 728 854 723 846 745 ...
##  $ arr_delay     : num  11 20 33 -18 -25 12 19 -14 -8 8 ...
##  $ carrier       : chr  "UA" "UA" "AA" "B6" ...
##  $ flight        : int  1545 1714 1141 725 461 1696 507 5708 79 301 ...
##  $ tailnum       : chr  "N14228" "N24211" "N619AA" "N804JB" ...
##  $ origin        : chr  "EWR" "LGA" "JFK" "JFK" ...
##  $ dest          : chr  "IAH" "IAH" "MIA" "BQN" ...
##  $ air_time      : num  227 227 160 183 116 150 158 53 140 138 ...
##  $ distance      : num  1400 1416 1089 1576 762 ...
##  $ hour          : num  5 5 5 5 6 5 6 6 6 6 ...
##  $ minute        : num  15 29 40 45 0 58 0 0 0 0 ...
##  $ time_hour     : POSIXct, format: "2013-01-01 05:00:00" ...
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC3.5)} What are some examples in this dataset of
\textbf{categorical} variables? What makes them different than
\textbf{quantitative} variables?

\textbf{(LC3.6)} What does \texttt{int}, \texttt{num}, and \texttt{chr}
mean in the output above?

\textbf{(LC3.7)} How many different columns are in this dataset?

\textbf{(LC3.8)} How many different rows are in this dataset?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Another way to view the properties of a dataset is to use the
\texttt{str} function (``str'' is short for ``structure''). This will
give you the first few entries of each variable in a row after the
variable. In addition, the type of the variable is given immediately
after the \texttt{:} following each variable's name. Here, \texttt{int}
and \texttt{num} refer to quantitative variables. In contrast,
\texttt{chr} refers to categorical variables. One more type of variable
is given here with the \texttt{time\_hour} variable: \textbf{POSIXct}.
As you may suspect, this variable corresponds to a specific date and
time of day.

Another nice feature of R is the help system. You can get help in R by
simply entering a question mark before the name of a function or an
object and you will be presented with a page showing the documentation.
Note that this output help file is omitted here but can be accessed
\href{https://cran.r-project.org/web/packages/nycflights13/nycflights13.pdf}{here}
on page 3 of the PDF document.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?flights}
\end{Highlighting}
\end{Shaded}

Another aspect of tidy data is a description of what each variable in
the dataset represents. This helps others to understand what your
variable names mean and what they correspond to. If we look at the
output of \texttt{?flights}, we can see that a description of each
variable by name is given.

An important feature to \textbf{ALWAYS} include with your data is the
appropriate units of measurement. We'll see this further when we work
with the \texttt{dep\_delay} variable in Chapter \ref{viz}. (It's in
minutes, but you'd get some really strange interpretations if you
thought it was in hours or seconds. UNITS MATTER!)

\section{\texorpdfstring{How is \texttt{flights}
tidy?}{How is flights tidy?}}\label{how-is-flights-tidy}

We see that \texttt{flights} has a rectangular shape with each row
corresponding to a different flight and each column corresponding to a
characteristic of that flight. This matches exactly with how Hadley
defined tidy data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable forms a column.
\item
  Each observation forms a row.
\end{enumerate}

But what about the third property?

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Each type of observational unit forms a table.
\end{enumerate}
\end{quote}

We identified earlier that the observational unit in the
\texttt{flights} dataset is an individual flight. And we have shown that
this dataset consists of 336776 flights with 19 variables. In other
words, some rows of this dataset don't refer to a measurement on an
airline or on an airport. They specifically refer to
characteristics/measurements on a given \textbf{flight} from New York
City in 2013.

By contrast, also included in the \texttt{nycflights13} package are
datasets with different observational units \citep{R-nycflights13}:

\begin{itemize}
\tightlist
\item
  \texttt{weather}: hourly meteorological data for each airport
\item
  \texttt{planes}: construction information about each plane
\item
  \texttt{airports}: airport names and locations
\item
  \texttt{airlines}: translation between two letter carrier codes and
  names
\end{itemize}

You may have been asking yourself what \texttt{carrier} refers to in the
\texttt{str(flights)} output above. The \texttt{airlines} dataset
provides a description of this with each airline being the observational
unit:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(airlines)}
\NormalTok{airlines}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 16 x 2
##    carrier                        name
##      <chr>                       <chr>
## 1       9E           Endeavor Air Inc.
## 2       AA      American Airlines Inc.
## 3       AS        Alaska Airlines Inc.
## 4       B6             JetBlue Airways
## 5       DL        Delta Air Lines Inc.
## 6       EV    ExpressJet Airlines Inc.
## 7       F9      Frontier Airlines Inc.
## 8       FL AirTran Airways Corporation
## 9       HA      Hawaiian Airlines Inc.
## 10      MQ                   Envoy Air
## 11      OO       SkyWest Airlines Inc.
## 12      UA       United Air Lines Inc.
## 13      US             US Airways Inc.
## 14      VX              Virgin America
## 15      WN      Southwest Airlines Co.
## 16      YV          Mesa Airlines Inc.
\end{verbatim}

As can be seen here when you just enter the name of an object in R, by
default it will print the contents of that object to the screen. Be
careful! It's usually better to use the \texttt{View()} function in
RStudio since larger objects may take awhile to print to the screen and
it likely won't be helpful to you to have hundreds of lines outputted.

\section{Normal forms of data}\label{normal-forms-of-data}

The datasets included in the \texttt{nycflights13} package are in a form
that minimizes redundancy of data. We will see that there are ways to
\emph{merge} (or \emph{join}) the different tables together easily. We
are capable of doing so because each of the tables have \emph{keys} in
common to relate one to another. This is an important property of
\textbf{normal forms} of data. The process of decomposing dataframes
into less redundant tables without losing information is called
\textbf{normalization}. More information is available on
\href{https://en.wikipedia.org/wiki/Database_normalization}{Wikipedia}.

We saw an example of this above with the \texttt{airlines} dataset.
While the \texttt{flights} dataframe could also include a column with
the names of the airlines instead of the carrier code, this would be
repetitive since there is a unique mapping of the carrier code to the
name of the airline/carrier.

Below an example is given showing how to \textbf{join} the
\texttt{airlines} dataframe together with the \texttt{flights} dataframe
by linking together the two datasets via a common \textbf{key} of
\texttt{"carrier"}. Note that this ``joined'' dataframe is assigned to a
new dataframe called \texttt{joined\_flights}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{joined_flights <-}\StringTok{ }\KeywordTok{inner_join}\NormalTok{(}\DataTypeTok{x =} \NormalTok{flights, }\DataTypeTok{y =} \NormalTok{airlines, }\DataTypeTok{by =} \StringTok{"carrier"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{View}\NormalTok{(joined_flights)}
\end{Highlighting}
\end{Shaded}

If we \texttt{View} this dataset, we see a new variable has been created
called (We will see in Subsection \ref{select} ways to change
\texttt{name} to a more descriptive variable name.)

More discussion about joining dataframes together will be given in
Chapter \ref{manip}. We will see there that the names of the columns to
be linked need not match as they did here with \texttt{"carrier"}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{review}
\textbf{\emph{Review questions}}
\end{review}

\textbf{(RQ3.1)} What are common characteristics of ``tidy'' datasets?

\textbf{(RQ3.2)} What makes ``tidy'' datasets useful for organizing
data?

\textbf{(RQ3.4)} How many variables are presented in the table below?
What does each row correspond to? (\textbf{Hint:} You may not be able to
answer both of these questions immediately but take your best guess.)

\begin{tabular}{r|r}
\hline
students & faculty\\
\hline
4 & 2\\
\hline
6 & 3\\
\hline
\end{tabular}

\textbf{(RQ3.5)} The confusion you may have encountered in Question 4 is
a common one those that work with data are commonly presented with. This
dataset is not tidy. Actually, the dataset in Question 4 has three
variables not the two that were presented. Make a guess as to what these
variables are and present a tidy dataset instead of this untidy one
given in Question 4.

\textbf{(RQ3.6)} The actual data presented in Question 4 is given below
in tidy data format:

\begin{tabular}{l|l|l}
\hline
role & Sociology? & Type of School\\
\hline
student & TRUE & Public\\
\hline
student & TRUE & Public\\
\hline
student & TRUE & Public\\
\hline
student & TRUE & Public\\
\hline
student & FALSE & Public\\
\hline
student & FALSE & Public\\
\hline
student & FALSE & Private\\
\hline
student & FALSE & Private\\
\hline
student & FALSE & Private\\
\hline
student & FALSE & Private\\
\hline
faculty & TRUE & Public\\
\hline
faculty & TRUE & Public\\
\hline
faculty & FALSE & Public\\
\hline
faculty & FALSE & Private\\
\hline
faculty & FALSE & Private\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  What does each row correspond to?\\
\item
  What are the different variables in this dataframe?\\
\item
  The \texttt{Sociology?} variable is known as a logical variable. What
  types of values does a logical variable take on?
\end{itemize}

\textbf{(RQ3.7)} What are some advantages of data in normal forms? What
are some disadvantages?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{What's to come?}\label{whats-to-come}

In Chapter \ref{viz}, we will further explore the distribution of a
variable in a related dataset to \texttt{flights}: the \texttt{temp}
variable in the \texttt{weather} dataset. We'll be interested in
understanding how this variable varies in relation to the values of
other variables in the dataset. We will see that visualization is often
a powerful tool in helping us see what is going on in a dataset. It will
be a useful way to expand on the \texttt{str} function we have seen here
for tidy data.

\chapter{Visualizing Data}\label{viz}

In Chapter \ref{tidy}, we discussed the importance of datasets being
\textbf{tidy}. You will see in examples here why having a tidy dataset
helps us immensely with plotting our data. In plotting our data, we will
be able to gain valuable insights from our data that we couldn't
initially see from just looking at the raw tidy data. We will focus on
using Hadley's \texttt{ggplot2} package in doing so, which was developed
to work specifically on datasets that are \textbf{tidy}. It provides an
easy way to customize your plots and is based on data visualization
theory given in \emph{The Grammar of Graphics} \citep{wilkinson2005}.

Graphics provide a nice way for us to get a sense for how quantitative
variables compare in terms of their center and their spread. The most
important thing to know about graphics is that they should be created to
make it obvious for your audience to see the findings you want to get
across. This requires a balance on not including too much in your plots,
but also including enough so that relationships and interesting findings
can be easily seen. As we will see, plots/graphics also help us to
identify patterns and outliers in our data. We will see that a common
extension of these ideas is to compare the distribution (i.e., what the
spread of a variable looks like) as we go across the levels of a
different categorical variable.

\section{Five Named Graphs - The FNG}\label{five-named-graphs---the-fng}

For our purposes here, we will be working with five different types of
graphs. (Note that we will use a lot of different words here in regards
to plotting - ``graphs'', ``plots'', and ``charts'' are all ways to
discuss a resulting graphic. You can think of them as all being
synonyms.) These five plots are:

\begin{itemize}
\tightlist
\item
  histograms
\item
  boxplots
\item
  barplots
\item
  scatter-plots
\item
  line-graphs
\end{itemize}

With this toolbox of plots, you can visualize just about any type of
variable thrown at you. We will discuss some other variations of these
but with the FNG in your repertoire you can do big things! Something we
will also stress here is that certain plots only work for
categorical/logical variables and others only for quantitative
variables. You'll want to quiz yourself often on which plot makes sense
with a given problem set-up.

We now introduce another dataframe in the \texttt{nycflights13} package
introduced in Chapter \ref{tidy}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nycflights13)}
\KeywordTok{data}\NormalTok{(weather)}
\end{Highlighting}
\end{Shaded}

\section{Histograms}\label{histograms}

Our focus now turns to the \texttt{temp} variable in this
\texttt{weather} dataset. We would like to visualize what the 26130
temperatures look like. Looking over the \texttt{weather}
dataset\footnote{Recall that to view a dataset in spreadsheet format in
  RStudio, you can run the \texttt{View()} function with the dataframe
  as its argument.} and running \texttt{?weather}, we can see that the
\texttt{temp} variable corresponds to hourly temperature (in Fahrenheit)
recordings at weather stations near airports in New York City. We could
just produce points where each of the different values appears on
something similar to a number line:

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-14-1} 

}

\caption[Strip Plot of Hourly Temperature Recordings from NYC in 2013]{Strip Plot of Hourly Temperature Recordings from NYC in 2013}\label{fig:unnamed-chunk-14}
\end{figure}

This gives us a general idea of how the values of \texttt{temp} differ.
We see that temperatures vary from around 11 up to 100 degrees
Fahrenheit. The area between 40 and 60 degrees appears to have more
points plotted than outside that range.

What is commonly produced instead of this strip plot is a plot known as
a \textbf{histogram}. The \textbf{histogram} show how many elements of
the variable fall in specified \textbf{bins}. These \textbf{bins} may
correspond to between 0-10°F, 10-20°F, etc.

To produce a histogram, we introduce the Hadley's \texttt{ggplot2}
package \citep{R-ggplot2}. We will use the \texttt{ggplot} function
which expects at a bare minimal as arguments

\begin{itemize}
\tightlist
\item
  the dataframe where the variables exist and
\item
  the names of the variables to be plotted.
\end{itemize}

The names of the variables will be entered into the \texttt{aes}
function as arguments where \texttt{aes} stands for ``aesthetics''.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{weather, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{temp))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-15-1} \end{center}

The plot given above is not a histogram, but the output does show us a
bit of what is going on with
\texttt{ggplot(data\ =\ weather,\ mapping\ =\ aes(x\ =\ temp))}. It is
producing a backdrop onto which we will ``paint'' elements.

We next proceed by adding a layer---hence, the use of the \texttt{+}
symbol---to the plot to produce a histogram. (Note also here that we
don't have to specify the \texttt{data\ =} and \texttt{mapping\ =} text
in our function calls. This is covered in more detail in Chapter 5 of
the ``Getting Used to R, RStudio, and R Markdown'' book
\citep{usedtor2016}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{weather, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{temp)) +}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{verbatim}
## Warning: Removed 1 rows containing non-finite values (stat_bin).
\end{verbatim}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-16-1} 

}

\caption[Histogram of Hourly Temperature Recordings from NYC in 2013]{Histogram of Hourly Temperature Recordings from NYC in 2013}\label{fig:unnamed-chunk-16}
\end{figure}

We have the power to specify how many bins we would like to put the data
into as an argument in the \texttt{geom\_histogram} function. By
default, this is chosen to be 30 somewhat arbitrarily and we have
received a warning above our plot that this was done. We also notice
here that another warning about 1 missing value is given. This value is
omitted from the plot. This warning is ignored for future customizations
of the plot.

Missing values are a common problem when working with data and there are
entire fields of study on how to work with missing data. Frequently what
is done is to remove instances from the data set that are missing. This
is certainly the easy way to deal with them, but not necessarily the
correct decision in all cases. For our purposes, we will usually ignore
potential warnings about missing data since R can usually handle them by
ignoring them.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{weather, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{temp)) +}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{60}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-17-1} 

}

\caption[Histogram of Hourly Temperature Recordings from NYC in 2013 - 60 Bins]{Histogram of Hourly Temperature Recordings from NYC in 2013 - 60 Bins}\label{fig:unnamed-chunk-17}
\end{figure}

We can tweak the plot a little more by specifying the width of the bins
(instead of how many bins to divide the variable into) by using the
\texttt{binwidth} argument in the \texttt{geom\_histogram} function. We
can also add some color to the plot by invoking the \texttt{fill} and
\texttt{color} arguments. A listing of all of the built-in colors to R
by name and color is available
\href{http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf}{here}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{weather, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{temp)) +}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{10}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"forestgreen"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-18-1} 

}

\caption[Histogram of Hourly Temperature Recordings from NYC in 2013 - Binwidth = 10]{Histogram of Hourly Temperature Recordings from NYC in 2013 - Binwidth = 10}\label{fig:unnamed-chunk-18}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.1)} What does changing the number of bins from 30 to 60
tell us about the distribution of temperatures?

\textbf{(LC4.2)} Would you classify the distribution of temperatures as
symmetric or skewed?

\textbf{(LC4.3)} What would you guess is the ``center'' value in this
distribution? Why did you make that choice?

\textbf{(LC4.4)} Is this data spread out greatly from the center or is
it close? Why?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Continuous data summaries}\label{contsum}

The \texttt{temp} variable is a \textbf{continuous} quantitative
variable (frequently just called a \textbf{continuous variable}). ``A
variable is \textbf{continuous} if you can arrange its values in order
and an infinite number of unique values can exist between any two values
of the variable''\citep{r4ds2016}. Some common examples of continuous
variables are time and height. Between any two times there are an
infinitely many number of time units that fall between them.

It is often easier to think about quantitative variables that are not
continuous to help us better understand continuity. The best example is
counts. If we are looking to count the number of flights that depart on
a given day from New York City, this variable would not be continuous.
It falls on a \textbf{discrete} scale.

We can examine some summary information about this \texttt{temp}
variable. To do so, we introduce the \texttt{summary} function. (We will
see in Chapter \ref{manip} how to use the \texttt{summarize} function in
the \texttt{dplyr} package to produce similar results.)

The syntax here is a little different than what we have seen before. (A
further discussion about R syntax is available in Chapter 5 of the ``R
basics'' book \citep{usedtor2016}). Here, \texttt{summary} is the
function and it is expecting an object to be summarized as its argument.
The object here is the \texttt{temp} variable in the \texttt{weather}
dataframe. To focus on just this one variable \texttt{temp} in
\texttt{weather}, we separate them by the dollar sign symbol
\texttt{\$}. Order matters here: the dataframe comes before the
\texttt{\$} and the variable/column name comes after.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(weather$temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##   10.94   39.92   55.04   55.20   69.98  100.00       1
\end{verbatim}

This tells us what is known as the \textbf{five-number summary} for our
variable as well as the \textbf{mean} value of the variable. More
information on both of these concepts is given in Appendix B - Chapter
\ref{appendix2}.

This \texttt{summary} gives us some numerical summaries of our
temperature variable. The minimum recorded temperature is 10.94 degrees
Fahrenheit and the maximum is 100.04 degrees Fahrenheit. We have one
missing value denoted as an \texttt{NA} in the observations of this
variable. The median Fahrenheit temperature of 55.04 and mean of
55.2035149 are quite close. This is a property of symmetric
distributions.

The last two entries given by \texttt{summary} correspond to the
25\textsuperscript{th} percentile and the 75\textsuperscript{th}
percentile. If we sorted all of the temperatures in increasing order, we
would see that 25\% of them would fall below 39.92 and that 75\% of them
would fall below 69.98. This implies that the middle 50\% of data values
lie between 39.92 and 69.98 degrees Fahrenheit.

Another common measure to determine the variability of a data set is the
\textbf{standard deviation}. You can read further about the mathematical
details of standard deviation in Appendix B - Chapter \ref{appendix2},
but you can think of it as being a measure of how far the data is, on
average, from the mean. Let's investigate this further by calculating
the standard deviation of our temperature variable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(weather$temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

Remember that there were some missing values in our temperature data
when we plotted it earlier. By default, the \texttt{sd} function that
computes standard deviation of a variable will give the value of
\texttt{NA} if any of the data is missing. To further understand this,
you can look at the help of the \texttt{sd} function in the R Console:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?sd}
\end{Highlighting}
\end{Shaded}

Under \textbf{Usage} you can see that \texttt{na.rm} is set to
\texttt{FALSE} by default. We'd like for missing values to be removed
from our analysis so we set that value to \texttt{TRUE}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(weather$temp, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 17.78212
\end{verbatim}

Standard deviation is always in the same units as our original data set.
So in this case the standard deviation is in degrees Fahrenheit. Thus,
we can interpret this value saying:

For a randomly selected element in our temperature column, we can expect
it to be about 17.7821236 degrees Fahrenheit from our mean value of
`55.2035149. By combining this knowledge with the plots above, we can
have a good idea for whether this data is very spread out from its
center or if it is tightly bunched.

\subsection{Summary}\label{summary}

Histograms provide a useful way of looking at how ONE continuous
variable varies. They allow us to answer questions such as

\begin{itemize}
\tightlist
\item
  Are there values far away from the center? These are commonly called
  \textbf{outliers} and can frequently be easily identified on a
  histogram.
\item
  Are most values close to the center? If so, the spread of the variable
  is small. If not, the spread is large.
\item
  How spread out are the values? One measure of this spread is
  \textbf{standard deviation} discussed above.
\end{itemize}

The histogram shows how many entries fall in different groupings of this
variable. Another common property of distributions is symmetry and as we
saw it is quite easily identified by looking over the histogram produced
from the variable's values.

\section{Boxplots}\label{boxplots}

Histograms can also be produced to compare the distribution of a
variable over another variable. Suppose we were interested in looking at
how the temperature recordings we saw in the last section varied by
month. This is what is meant by ``the distribution of a variable over
another variable'': \texttt{temp} is one variable and \texttt{month} is
the other variable.

\subsection{Faceting}\label{faceting}

In order to look at histograms of \texttt{temp} for each month, we
introduce a new concept called \textbf{facetting}. Faceting is used when
we'd like to create small multiples of the same plot over a different
categorical variable. By default, all of the small multiples will have
the same vertical axis. An example will help here. We will discuss the
concept of faceting in further detail in Section \ref{barplots}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{weather, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{temp)) +}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{5}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"firebrick"}\NormalTok{) +}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~month)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-23-1} \end{center}

As we might expect, the temperature tends to increase as summer
approaches and then decrease as winter approaches.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.5)} What other things do you notice about the faceted plot
above? How does a faceted plot help us see how relationships between two
variables?

\textbf{(LC4.6)} What do the numbers 1-12 correspond to in the plot
above? What about 25, 50, 75, 100?

\textbf{(LC4.7)} What could be done to make the faceted plot above more
readable? (Focus on tweaking the histograms and not on making a
different type of plot here.)

\textbf{(LC4.8)} For which types of datasets would these types of
faceted plots not work well in comparing relationships between
variables? Draw or give an example.

\textbf{(LC4.9)} Does the \texttt{temp} variable in the \texttt{weather}
data set have a lot of variability? Why do you say that?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Histograms can provide a way to compare distributions across groups as
we see above when we looked at temperature over months. Frequently, a
plot called a \textbf{boxplot} (also called a \textbf{side-by-side
boxplot}) is done instead. The \textbf{boxplot} uses the information
provided in the \textbf{five-number summary} referred to in the previous
section when we used the \texttt{summary} function. It gives a way to
compare this summary information across the different levels of a group.
Let's create a boxplot to compare the monthly temperatures as we did
above with the faceted histograms.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{weather, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{month, }\DataTypeTok{y =} \NormalTok{temp)) +}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Continuous x aesthetic -- did you forget aes(group=...)?
\end{verbatim}

\begin{verbatim}
## Warning: Removed 1 rows containing non-finite values (stat_boxplot).
\end{verbatim}

\begin{center}\includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-24-1} \end{center}

Note the first warning that is given here. (The second one corresponds
to missing values in the dataframe and it is turned off on subsequent
plots.) This plot does not look like what we were expecting. We were
expecting to see the distribution of temperatures for each month (so 12
different boxplots). This gives us the overall boxplot without any other
groupings. We can get around this by introducing a new function for our
\texttt{x} variable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{weather, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(month), }\DataTypeTok{y =} \NormalTok{temp)) +}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-25-1} \end{center}

We have introduced a new function called \texttt{factor()} here. One of
the things this function does is to convert a numeric value like
\texttt{month} (1, 2, \ldots{}, 12) into a categorical variable. The
``box'' part of this plot represents the 25\textsuperscript{th}
percentile, the median (50\textsuperscript{th} percentile), and the
75\textsuperscript{th} percentile. The dots correspond to
\textbf{outliers}. (The specific formulation for these outliers is
discussed in Appendix B - Chapter \ref{appendix2}.) The lines show how
the data varies that is not in the center 50\% defined by the first and
third quantiles. Longer lines correspond to more variability and shorter
lines correspond to less variability.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.10)} What does the dot at the bottom of the plot for May
correspond to? Explain what might have occurred in May to produce this
point.

\textbf{(LC4.11)} Which months have the highest variability in
temperature? What reasons do you think this is?

\textbf{(LC4.12)} We looked at the distribution of a continuous variable
over a categorical variable here with this boxplot. Why can't we look at
the distribution of one continuous variable over the distribution of
another continuous variable? Say temperature across pressure, for
example?

\textbf{(LC4.13)} Boxplots provide a simple way to identify outliers.
Why may outliers be easier to identify when looking at a boxplot instead
of a faceted histogram?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Summary}\label{summary-1}

Boxplots provide a way to compare and contrast the distribution of ONE
quantitative variable across multiple levels of ONE categorical
variable. One can easily look to see where the median falls across the
different groups by looking at the center line in the box. You can also
see how spread out the variable is across the different groups by
looking at the width of the box and also how far out the lines stretch
from the box. Lastly, outliers are even more easily identified when
looking at a boxplot than when looking at a histogram.

\section{Barplots}\label{barplots}

Both histograms and boxplots represent ways to visualize the variability
of continuous variables. Another common task is to present the
distribution of a categorical variable. This is a simpler task since we
will be interested in how many elements from our data fall into the
different categories of the categorical variable. We need not bin the
data or identify the different quantiles for categorical variables.

Frequently, the best way to visualize these different counts (also known
as \textbf{frequencies}) is via a barplot. Consider the distribution of
airlines that flew out of New York City in 2013. This can be plotted by
invoking the \texttt{geom\_bar} function in \texttt{ggplot2}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{flights, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carrier)) +}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-26-1} 

}

\caption[Number of flights departing NYC in 2013 by airline]{Number of flights departing NYC in 2013 by airline}\label{fig:unnamed-chunk-26}
\end{figure}

Recall the \texttt{airlines} dataset discussed in Chapter \ref{tidy}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(airlines)}
\NormalTok{airlines}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 16 x 2
##    carrier                        name
##      <chr>                       <chr>
## 1       9E           Endeavor Air Inc.
## 2       AA      American Airlines Inc.
## 3       AS        Alaska Airlines Inc.
## 4       B6             JetBlue Airways
## 5       DL        Delta Air Lines Inc.
## 6       EV    ExpressJet Airlines Inc.
## 7       F9      Frontier Airlines Inc.
## 8       FL AirTran Airways Corporation
## 9       HA      Hawaiian Airlines Inc.
## 10      MQ                   Envoy Air
## 11      OO       SkyWest Airlines Inc.
## 12      UA       United Air Lines Inc.
## 13      US             US Airways Inc.
## 14      VX              Virgin America
## 15      WN      Southwest Airlines Co.
## 16      YV          Mesa Airlines Inc.
\end{verbatim}

We see that United Air Lines, JetBlue Airways, and ExpressJet Airlines
had the most flights depart New York City in 2013. To get the actual
number of flights by each airline we can use the \texttt{count} function
in the \texttt{dplyr} package on the \texttt{carrier} variable in
\texttt{flights}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{flights_table <-}\StringTok{ }\KeywordTok{count}\NormalTok{(}\DataTypeTok{x =} \NormalTok{flights, }\DataTypeTok{vars =} \NormalTok{carrier)}
\NormalTok{flights_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 16 x 2
##     vars     n
##    <chr> <int>
## 1     9E 18460
## 2     AA 32729
## 3     AS   714
## 4     B6 54635
## 5     DL 48110
## 6     EV 54173
## 7     F9   685
## 8     FL  3260
## 9     HA   342
## 10    MQ 26397
## 11    OO    32
## 12    UA 58665
## 13    US 20536
## 14    VX  5162
## 15    WN 12275
## 16    YV   601
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.14)} Why are histograms inappropriate for visualizing
categorical variables?

\textbf{(LC4.15)} What is the difference between histograms and
barplots?

\textbf{(LC4.16)} How many Envoy Air flights departed NYC in 2013?

\textbf{(LC4.17)} What was the seventh highest airline in terms of
departed flights from NYC in 2013?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Must avoid pie charts!}\label{must-avoid-pie-charts}

Unfortunately, one of the most common plots seen today for categorical
data is the pie chart. While they may see harmless enough, they actually
present a problem in that humans are unable to judge angles well. As
Naomi Robbins describes in her book ``Creating More Effective Graphs'',
we overestimate angles greater than 90 degrees and we underestimate
angles less than 90 degrees. In other words, it is difficult for us to
determine relative size of one piece of the pie compared to another.

Let's examine our previous barplot example on the number of flights
departing NYC by airline. This time we will use a pie chart. As you
review this chart, try to identify

\begin{itemize}
\tightlist
\item
  how much larger the portion of the pie is for ExpressJet Airlines
  (\texttt{EV}) compared to US Airways (\texttt{US}),
\item
  what the third largest carrier is in terms of departing flights, and
\item
  how many carriers have fewer flights than United Airlines
  (\texttt{UA})?
\end{itemize}

\begin{center}\includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-29-1} \end{center}

While it is quite easy to look back at the barplot to get the answer to
these questions, it's quite difficult to get the answers correct when
looking at the pie graph. Barplots can always present the information in
a way that is easier for the eye to determine relative position. There
may be one exception from Nathan Yau at
\href{https://flowingdata.com/2008/09/19/pie-i-have-eaten-and-pie-i-have-not-eaten/}{FlowingData.com}
but we will leave this for the reader to decide:

\begin{center}\includegraphics{images/Pie-I-have-Eaten} \end{center}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.18)} Why should pie charts be avoided and replaced by
barplots?

\textbf{(LC4.19)} What is your opinion as to why pie charts continue to
be used?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Using barplots to compare two
variables}\label{using-barplots-to-compare-two-variables}

Barplots are the go-to way to visualize the frequency of different
categories of a categorical variable. They make it easy to order the
counts and to compare one group's frequency to another. Another use of
barplots (unfortunately, sometimes inappropriately and confusingly) is
to compare two categorical variables together. Let's examine the
distribution of outgoing flights from NYC by \texttt{carrier} and
\texttt{airport}.

We begin by getting the names of the airports in NYC that were included
in the \texttt{flights} dataset. Remember from Chapter \ref{tidy} that
this can be done by using the \texttt{inner\_join} function in the
\texttt{dplyr} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{flights_namedports <-}\StringTok{ }\KeywordTok{inner_join}\NormalTok{(flights, airports, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"origin"} \NormalTok{=}\StringTok{ "faa"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

After running \texttt{View(flights\_namedports)}, we see that
\texttt{name} now corresponds to the name of the airport as referenced
by the \texttt{origin} variable. We will now plot \texttt{carrier} as
the horizontal variable. When we specify \texttt{geom\_bar}, it will
specify \texttt{count} as being the vertical variable. A new addition
here is \texttt{fill\ =\ name}. Look over what was produced from the
plot to get an idea of what this argument gives.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{flights_namedports, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carrier, }\DataTypeTok{fill =} \NormalTok{name)) +}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-32-1} 

}

\caption[Stacked barplot comparing the number of flights by carrier and airport]{Stacked barplot comparing the number of flights by carrier and airport}\label{fig:unnamed-chunk-32}
\end{figure}

This plot is what is known as a \textbf{stacked barplot}. While simple
to make, it often leads to many problems.

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.20)} What kinds of questions are not easily answered by
looking at the above figure?

\textbf{(LC4.21)} What can you say, if anything, about the relationship
between airline and airport in NYC in 2013 in regards to the number of
departing flights?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Another variation on the \textbf{stacked barplot} is the
\textbf{side-by-side barplot}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{flights_namedports, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carrier, }\DataTypeTok{fill =} \NormalTok{name)) +}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-33-1} 

}

\caption[Side-by-side barplot comparing the number of flights by carrier and airport]{Side-by-side barplot comparing the number of flights by carrier and airport}\label{fig:unnamed-chunk-33}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.22)} Why might the side-by-side barplot be preferable to a
stacked barplot in this case?

\textbf{(LC4.23)} What are the disadvantages of using a side-by-side
barplot, in general?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Lastly, an often preferred type of barplot is the \textbf{faceted
barplot}. We already saw this concept of faceting and small multiples in
Subsection \ref{faceting}. This gives us a nicer way to compare the
distributions across both \texttt{carrier} and airport/\texttt{name}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{flights_namedports, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carrier, }\DataTypeTok{fill =} \NormalTok{name)) +}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() +}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(name ~}\StringTok{ }\NormalTok{.)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-34-1} 

}

\caption[Faceted barplot comparing the number of flights by carrier and airport]{Faceted barplot comparing the number of flights by carrier and airport}\label{fig:unnamed-chunk-34}
\end{figure}

Note how the \texttt{facet\_grid} function arguments are written here.
We are wanting the names of the airports vertically and the
\texttt{carrier} listed horizontally. As you may have guessed, this
argument and other \emph{formulas} of this sort in R are in
\texttt{y\ \textasciitilde{}\ x} order. We will see more examples of
this in Chapter \ref{infer}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.24)} Why is the faceted barplot preferred to the
side-by-side and stacked barplots in this case?

\textbf{(LC4.25)} What information about the different carriers at
different airports is more easily seen in the faceted barplot?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Summary}\label{summary-2}

Barplots are the preferred way of displaying categorical variables. They
are easy-to-understand and to make comparisons across groups of a
categorical variable. When dealing with more than one categorical
variable, faceted barplots are frequently preferred over side-by-side or
stacked barplots. Stacked barplots are sometimes nice to look at, but it
is quite difficult to compare across the levels since the sizes of the
bars are all of different sizes. Side-by-side barplots can provide an
improvement on this, but the issue about comparing across groups still
must be dealt with.

\section{Scatter-plots}\label{scatter-plots}

We have seen that boxplots are most appropriate when plotting the
distribution of ONE continuous variable across different levels/groups
of ONE categorical variable. Barplots (preferably the faceted type) are
best when looking at the distribution of ONE categorical variable across
different levels of another categorical variable. But what if we are
looking to investigate the relationship between TWO continuous
variables? What is commonly produced is the well-known
\textbf{scatter-plot}, which shows the points corresponding to the
values of each of the variables scattered around.

We will now investigate arrival delays (the vertical ``y'' axis
variable) versus departure delays (the horizontal ``x'' axis variable)
for Alaska Airlines flights leaving NYC in 2013. Notice the new function
that is invoked here: \texttt{filter}, which resides in the
\texttt{dplyr} package. You will see many more examples using this
function in Chapter \ref{manip}. The \texttt{filter} function goes
through the dataframe specified (\texttt{flights} here) and selects only
those rows which meet the condition given (\texttt{carrier\ ==\ "AS"}
here).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alaska_flights <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights, carrier ==}\StringTok{ "AS"}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(alaska_flights, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{dep_delay, }\DataTypeTok{y =} \NormalTok{arr_delay)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/noalpha-1} 

}

\caption[Arrival Delays vs Departure Delays for Alaska Airlines flights from NYC in 2013]{Arrival Delays vs Departure Delays for Alaska Airlines flights from NYC in 2013}\label{fig:noalpha}
\end{figure}

We see that a positive relationship exists between \texttt{dep\_delay}
and \texttt{arr\_delay}: as departure delays increase, arrival delays
tend to also increase. We also note that the majority of points fall
near the point (0, 0) here. There is a large mass of points clustered
there.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.26)} What are some practical reasons why
\texttt{dep\_delay} and \texttt{arr\_delay} have a positive
relationship?

\textbf{(LC4.27)} What variables (not necessarily in the
\texttt{flights} dataframe) would you expect to have a negative
correlation (i.e.~a negative relationship) with \texttt{dep\_delay}?
Why? Remember that we are focusing on continuous variables here.

\textbf{(LC4.28)} Why do you believe there is a cluster of points near
(0, 0)?

\begin{itemize}
\tightlist
\item
  What does (0, 0) correspond to in terms of the Alaskan flights?
\end{itemize}

\textbf{(LC4.29)} What are some other features of the plot that stand
out to you?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Jittering}\label{jittering}

The large mass of points near (0, 0) can cause some confusion. This is
the result of a phenomenon called \textbf{over-plotting}. As one may
guess, this corresponds to values being plotted on top of each other
\emph{over} and \emph{over} again. It is often difficult to know just
how many values are plotted in this way when looking at a basic
scatter-plot as we have here.

One way of relieving this issue of \textbf{over-plotting} is to
\textbf{jitter} the points a bit. In other words, we are going to add
just a bit of random noise to the points to better see them and remove
some of the over-plotting. You can think of ``jittering'' as shaking the
points a bit on the plot. Instead of using \texttt{geom\_point}, we use
\texttt{geom\_jitter} to perform this shaking and specify around how
much jitter to add with the \texttt{width} and \texttt{height}
arguments. This corresponds to how hard you'd like to shake the plot in
units corresponding to those for both the horizontal and vertical
variables (minutes here).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(alaska_flights, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{dep_delay, }\DataTypeTok{y =} \NormalTok{arr_delay)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =} \DecValTok{30}\NormalTok{, }\DataTypeTok{height =} \DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-36-1} \end{center}

This has helps us a little bit in getting a sense for the over-plotting,
but with a relatively large dataset like this one (714 flights), it is
often useful to change the transparency of the points as seen in the
next section.

\subsection{Setting transparency}\label{setting-transparency}

One of the arguments that can be changed with \texttt{geom\_point} is
\texttt{alpha}. By default, this value is set to \texttt{1}. We can
change this value to a smaller fraction to change the transparency of
the points in the plot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(alaska_flights, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{dep_delay, }\DataTypeTok{y =} \NormalTok{arr_delay)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/alpha-1} 

}

\caption[Arrival Delays vs Departure Delays for Alaska Airlines flights from NYC in 2013 - alpha=0.2]{Arrival Delays vs Departure Delays for Alaska Airlines flights from NYC in 2013 - alpha=0.2}\label{fig:alpha}
\end{figure}

We can also specify the \texttt{alpha} argument in
\texttt{geom\_jitter}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(alaska_flights, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{dep_delay, }\DataTypeTok{y =} \NormalTok{arr_delay)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =} \DecValTok{30}\NormalTok{, }\DataTypeTok{height =} \DecValTok{30}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/jitteralpha-1} 

}

\caption[Arrival Delays vs Departure Delays for Alaska Airlines flights from NYC in 2013 - jitter and alpha added]{Arrival Delays vs Departure Delays for Alaska Airlines flights from NYC in 2013 - jitter and alpha added}\label{fig:jitteralpha}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.30)} Why is setting the \texttt{alpha} argument value
useful with scatter-plots?

\begin{itemize}
\tightlist
\item
  What further information does it give you that a regular scatter-plot
  cannot?
\end{itemize}

\textbf{(LC4.31)} After viewing the \ref{fig:alpha} above, give a range
of arrival times and departure times that occur most frequently?

\begin{itemize}
\tightlist
\item
  How has that region changed compared to when you observed the same
  plot without the \texttt{alpha\ =\ 0.2} set in \ref{fig:noalpha}?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Maybe include a shading of the points by another variable
example here for multivariate thinking?}

\subsection{Summary}\label{summary-3}

Scatter-plots may be the most used plot today and they can provide an
immediate way to see the trend in one variable versus another. Remember
that they only make sense when plotting a continuous variable versus a
continuous variable though. If you try to create a scatter-plot where
either one of the two variables is not quantitative, you will get
strange results. Be careful!

With medium to large datasets, you may need to tweak arguments in both
\texttt{geom\_jitter} and the \texttt{alpha} parameter in order to get a
good feel for relationships in your data. This tweaking is often a fun
part of data visualization since you'll have the chance to see different
relationships come about as you make subtle changes to your plots.

\section{Line-graphs}\label{line-graphs}

The last of the FNG is a line-graph. They are most frequently used when
the horizontal axis is time. Time represents a variable that is
connected together by each day following the previous day. In other
words, time has a natural ordering. Line-graphs should be avoided when
there is not a clear ordering to the explanatory (``x'' variable).

We are interested in exploring the arrival delays by day throughout the
year of 2013 from outgoing flights from New York City. If we plotted all
of these values, we obtain the following scatter-plot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(flights, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{time_hour, }\DataTypeTok{y =} \NormalTok{arr_delay)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-37-1} \end{center}

We see that this plot is difficult to understand based on the sheer
number of points plotted. We see some outlier points with more than 500
minutes of arrival delay, but even some jittering and transparency is
not going to help us here.

Instead of plotting all of the values for each hour for all flights, it
might make sense to plot the average value for each day in terms of
arrival delays. Of course, we also need to address which average we
should use: mean or median. With there being some outliers here, we have
chosen to use the median arrival delay (since the mean is heavily
influence by outliers).

You may think that this is a difficult task but the \texttt{group\_by}
and \texttt{summarize} functions make this a breeze. You'll see more
examples using these two functions in Chapter \ref{manip}. Here we will
create a new variable, which corresponds to the month and day combined,
from the \texttt{time\_hour} column using the \texttt{mutate} function
and create a new dataframe called \texttt{flights\_day}. Notice from
running \texttt{View(flights\_day)} that the new variable added called
\texttt{date} appears on the far right of the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_day <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(flights, }\DataTypeTok{date =} \KeywordTok{as.Date}\NormalTok{(time_hour))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_summarized <-}\StringTok{ }\NormalTok{flights_day %>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(date) %>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{median_arr_delay =} \KeywordTok{median}\NormalTok{(arr_delay, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{flights_summarized}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 365 x 2
##          date median_arr_delay
##        <date>            <dbl>
## 1  2013-01-01                3
## 2  2013-01-02                4
## 3  2013-01-03                1
## 4  2013-01-04               -8
## 5  2013-01-05               -7
## 6  2013-01-06               -1
## 7  2013-01-07              -10
## 8  2013-01-08               -7
## 9  2013-01-09               -6
## 10 2013-01-10              -11
## # ... with 355 more rows
\end{verbatim}

You will see the ``pipe'' operator \texttt{\%\textgreater{}\%} explained
in more detail in Chapter \ref{manip}, but you can read it as ``and
then''. Here, we take the \texttt{flights\_day} dataframe that we just
created and then group it together by \texttt{date}. This goes through
the dataframe and puts together all rows that have \texttt{2013-01-01}
together, all rows that have \texttt{2013-01-02} together, \ldots{}, and
all rows that have \texttt{2013-12-31} together. And then it looks at
the median value of \texttt{arr\_delay} over each one of the days. You
can get a glimpse of the first few rows of this new dataset above since
we invoked the \texttt{head} function on it.

Note also that there are missing values in this data set so we need to
exclude them from the analysis. This is why the \texttt{na.rm\ =\ TRUE}
argument is invoked. Many functions require this extra specification so
it's always a good idea to run a \texttt{?median} or \texttt{?mean}
before you try to run the function. Or you can always run it afterwards
as well when you get strange results.

Now getting back to our line-graph. We want to plot the median arrival
delay over all airlines on all days in 2013 from departing flights in
NYC. This syntax should look similar to what we have seen before with
plots involving \texttt{ggplot}. Notice that we are using the
\texttt{flights\_summarized} dataset here and not the
\texttt{flights\_day} or \texttt{flights} dataframes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{flights_summarized, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{date, }\DataTypeTok{y =} \NormalTok{median_arr_delay)) +}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/lineflights-1} 

}

\caption[Line-graph of median arrival delay for flights leaving NYC in 2013 versus day of the year]{Line-graph of median arrival delay for flights leaving NYC in 2013 versus day of the year}\label{fig:lineflights}
\end{figure}

\subsection{Interactive line-graphs}\label{interactive-line-graphs}

Another useful tool for viewing line-graphs such as this is the
\texttt{dygraph} function in the \texttt{dygraphs} package in
combination with the \texttt{dyRangeSelector} function. This allows us
to zoom in on a selected range and get an interactive plot for us to
work with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dygraphs)}
\KeywordTok{rownames}\NormalTok{(flights_summarized) <-}\StringTok{ }\NormalTok{flights_summarized$date}
\NormalTok{flights_summarized <-}\StringTok{ }\KeywordTok{select}\NormalTok{(flights_summarized, -date)}
\KeywordTok{dyRangeSelector}\NormalTok{(}\KeywordTok{dygraph}\NormalTok{(flights_summarized))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-40-1} \end{center}

The syntax here is a little different than what we have covered so far.
The \texttt{dygraph} function is expecting for the dates to be given as
the \texttt{rownames} of the object. We then remove the \texttt{date}
variable from the \texttt{flights\_summarized} dataframe since it is
accounted for in the \texttt{rownames}. Lastly, we run the
\texttt{dygraph} function on the new dataframe that only contains the
median arrival delay as a column and then provide the ability to have a
selector to zoom in on the interactive plot via
\texttt{dyRangeSelector}. (Note that this plot will only be interactive
in the HTML version of this book.)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC4.32)} Why should line-graphs be avoided when there is not a
clear ordering of the horizontal axis?

\textbf{(LC4.33)} Why are line-graphs frequently used when time is the
explanatory variable?

\textbf{(LC4.34)} Why did we use the \texttt{flights\_summarized}
dataframe to produce the line-graph in Figure \ref{fig:lineflights}
instead of \texttt{flights} or \texttt{flights\_day}?

\textbf{(LC4.35)} Are the largest median arrival delays where you
expected them to occur on the line-graph above in Figure
\ref{lineflights}?

\textbf{(LC4.36)} Use the interactive line-graph to determine the
highest median arrival delay for flights from NYC in 2013. What date was
it and what do you think contributed to it?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Summary}\label{summary-4}

Line-graphs provide a useful tool for viewing a continuous variable that
is plotted versus time. We need to be careful to not be too entrenched
in using line-graphs whenever we wish though. They only make sense when
the explanatory variable (the one on the explanatory variable) has a
natural ordering. We can mislead our audience if that isn't the case.

\section{Brief Review of The Grammar of
Graphics}\label{brief-review-of-the-grammar-of-graphics}

The FNG discussion above has introduced you to all of the major pieces
behind ``The Grammar of Graphics'', which serves as the basis for the
\texttt{ggplot2} package. This theoretical framework given by Leland
Wilkinson \citep{wilkinson2005} helps us identify what the pieces are
that make up a statistical graphic:

\begin{quote}
In brief, the grammar tells us that a statistical graphic is a mapping
from data to aesthetic attributes (color, shape, size) of geometric
objects (points, lines, bars).
\end{quote}

Specially, we can break a graphic into the following components:

\begin{itemize}
\item
  \texttt{aes}: mappings of data to \emph{aesthetics} we can perceive on
  a graphic. These include x/y position, color, size, and shape. Each
  aesthetic can be mapped to a variable in our data set. If not
  assigned, they are set to defaults.
\item
  \texttt{geom}: (geometric objects) This refers to our type of plot:
  points, lines, bars, etc.
\item
  \texttt{stat}: (statistical transformations to summarize data) This
  includes smoothing, binning values into a histogram, or just itself
  \texttt{"identity"}. You'll see this when we get to regression later
  in Chapter \ref{regress}.
\item
  \texttt{facet}: how to break up data into subsets and display broken
  down plots as small multiples
\item
  \texttt{scales} both

  \begin{itemize}
  \tightlist
  \item
    convert \textbf{data units} to \textbf{physical units} the computer
    can display
  \item
    draw a legend and/or axes, which provide an inverse mapping to make
    it possible to read the original data values from the graph.
  \end{itemize}
\item
  \texttt{coord}: coordinate system for x/y values: typically cartesian.
\item
  \texttt{position} adjustments
\end{itemize}

There are other extra attributes that can be tweaked as well including
titles for the plot and each of the axes and also over-arching themes
for the plot. In general, the Grammar of Graphics allows for
customizability but also keeping with a consistent framework that allows
the user to easily tweak their creations as they wish or need to in
order to convey a message about their data.

We'll see (and have seen) that you don't necessarily need to include all
of these in your code to produce a plot but each of the components are
set by default and do exist with each plot produced using
\texttt{ggplot2}.

An excellent resource as you begin to create plots using the
\texttt{ggplot2} package is a cheatsheet that RStudio has put together
entitled ``Data Visualization with ggplot2'' available
\href{https://www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf}{here}.
This covers more than what we've discussed in this chapter but provides
nice visual descriptions of what each function produces.

\section{What's to come?}\label{whats-to-come-1}

In Chapter \ref{manip}, we'll further explore data by grouping our data,
creating summaries based on those groupings, filtering our data to match
conditions, selecting specific columns of our data, and other
manipulations with our data including defining new columns/variables.
These data manipulation procedures will go hand-in-hand with the data
visualizations you've produced here.

\chapter{Manipulating Data}\label{manip}

Let's briefly recap where we have been so far and where we are headed.
In Chapter \ref{tidy}, we discussed what it means for data to be tidy.
We saw that this corresponds to observations to correspond to rows and
for variables to be stored in columns. The entries in the data frame
correspond to different combinations of observational units and
variables. In the \texttt{flights} data frame, we saw that each row
corresponded to a different flight leaving New York City. (In other
words, the observational unit of that tidy data frame is a flight.) The
variables are listed as columns and for \texttt{flights} they include
both quantitative variables like \texttt{dep\_delay} and
\texttt{distance} but also categorical variables like \texttt{carrier}
and \texttt{origin}. An entry in the table corresponds to a particular
flight and a particular value of a given variable representing that
flight.

We saw in Chapter \ref{viz} that organizing data in this tidy way makes
it easy for use to produce graphics. We can simply specify what
variable/column we would like on one axis, what variable we'd like on
the other axis, and what type of plot we'd like to make. We can also do
things such as changing the color by another variable or change the size
of our points by a fourth variable given this tidy data set.

In Chapter \ref{viz}, we also introduced some ways to summarize and
manipulate data to suit your needs. This chapter focuses more on the
details of this by giving a variety of examples using the five main
verbs in the \texttt{dplyr} package. There are more advanced operations
that can be done than these and you'll see some examples of this near
the end of the chapter.

As we saw with the RStudio cheatsheet on
\href{https://www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf}{data
visualization}, RStudio has also created a cheatsheet for data
manipulation entitled ``Data Wrangling with dplyr and tidyr'' available
\href{https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}{here}.
We will focus only on the \texttt{dplyr} functions in this book, but you
are encouraged to also explore \texttt{tidyr} if you are presented with
data that is not in the tidy format that we have specified as the
preferred option for our purposes.

\section{Five Main Verbs - The FMV}\label{five-main-verbs---the-fmv}

If you scan over the Data Wrangling cheatsheet, you may be initially
overwhelmed by the amount of functions available. You'll see the use of
all of these as you work more and more with data frames in R. The
\texttt{d} in \texttt{dplyr} stands for data frames so the functions
here work when you are working with objects of the data frame type.

It's most important for you to focus on the five most commonly used
functions that help us manipulate and summarize data. A description of
these verbs follows with each subsection devoted to seeing an example of
that verb in play (or a combination of a few verbs):

\begin{itemize}
\tightlist
\item
  \texttt{select}: Choose variables/columns by their names
\item
  \texttt{filter}: Pick rows based on conditions about their values
\item
  \texttt{summarize}: Create summary measures of variables (or groups of
  observations on variables using \texttt{group\_by})
\item
  \texttt{mutate}: Make a new variable in the data frame
\item
  \texttt{arrange}: Sort the rows based on one or more variables
\end{itemize}

Just as we had the FNG (The Five Named Graphs in Chapter \ref{viz} using
\texttt{ggplot2}), we have the FMV here (The Five Main Verbs in
\texttt{dplyr}):

\subsection{\texorpdfstring{Select variables using
\texttt{select}}{Select variables using select}}\label{select-variables-using-select}

\begin{figure}

{\centering \includegraphics[width=16.06in]{images/select} 

}

\caption[Select diagram from Data Wrangling with dplyr and tidyr cheatsheet]{Select diagram from Data Wrangling with dplyr and tidyr cheatsheet}\label{fig:selectfig}
\end{figure}

We've seen that the \texttt{flights} data frame in the
\texttt{nycflights13} package contains many different variables (19 in
fact). You can identify this by running the \texttt{dim} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nycflights13)}
\KeywordTok{data}\NormalTok{(flights)}
\KeywordTok{dim}\NormalTok{(flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 336776     19
\end{verbatim}

One of these variables is \texttt{year}. If you remember the original
description of the \texttt{flights} data frame (or by running
\texttt{?flights}), you'll remember that this data correspond to flights
in 2013 departing New York City. The \texttt{year} variable isn't really
a variable here in that it doesn't vary\ldots{} \texttt{flights}
actually comes from a larger data set that covers many years. We may
want to remove the \texttt{year} variable from our data set. To do so
easily, we use the \texttt{select} variable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{flights <-}\StringTok{ }\KeywordTok{select}\NormalTok{(}\DataTypeTok{.data =} \NormalTok{flights, -year)}
\KeywordTok{names}\NormalTok{(flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "month"          "day"            "dep_time"       "sched_dep_time"
##  [5] "dep_delay"      "arr_time"       "sched_arr_time" "arr_delay"     
##  [9] "carrier"        "flight"         "tailnum"        "origin"        
## [13] "dest"           "air_time"       "distance"       "hour"          
## [17] "minute"         "time_hour"
\end{verbatim}

The \texttt{names} function gives a listing of all the columns in a data
frame. We see that \texttt{year} has been removed. This was done using a
\texttt{-} in front of the name of the column we'd like to remove.

We could also select specific columns (instead of deselecting columns)
by listing them out:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flight_dep_times <-}\StringTok{ }\KeywordTok{select}\NormalTok{(flights, month, day, dep_time, sched_dep_time)}
\NormalTok{flight_dep_times}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 336,776 x 4
##    month   day dep_time sched_dep_time
##    <int> <int>    <int>          <int>
## 1      1     1      517            515
## 2      1     1      533            529
## 3      1     1      542            540
## 4      1     1      544            545
## 5      1     1      554            600
## 6      1     1      554            558
## 7      1     1      555            600
## 8      1     1      557            600
## 9      1     1      557            600
## 10     1     1      558            600
## # ... with 336,766 more rows
\end{verbatim}

Or we could specify a ranges of columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flight_arr_times <-}\StringTok{ }\KeywordTok{select}\NormalTok{(flights, month:day, arr_time:sched_arr_time)}
\NormalTok{flight_arr_times}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 336,776 x 4
##    month   day arr_time sched_arr_time
##    <int> <int>    <int>          <int>
## 1      1     1      830            819
## 2      1     1      850            830
## 3      1     1      923            850
## 4      1     1     1004           1022
## 5      1     1      812            837
## 6      1     1      740            728
## 7      1     1      913            854
## 8      1     1      709            723
## 9      1     1      838            846
## 10     1     1      753            745
## # ... with 336,766 more rows
\end{verbatim}

The \texttt{select} function can also be used to reorder columns in
combination with the \texttt{everything} helper function. Let's suppose
we'd like the \texttt{hour}, \texttt{minute}, and \texttt{time\_hour}
variables, which appear at the end of the \texttt{flights} data set to
actually appear immediately after the \texttt{day} variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_reorder <-}\StringTok{ }\KeywordTok{select}\NormalTok{(flights, month:day, hour:time_hour, }\KeywordTok{everything}\NormalTok{())}
\KeywordTok{names}\NormalTok{(flights_reorder)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "month"          "day"            "hour"           "minute"        
##  [5] "time_hour"      "dep_time"       "sched_dep_time" "dep_delay"     
##  [9] "arr_time"       "sched_arr_time" "arr_delay"      "carrier"       
## [13] "flight"         "tailnum"        "origin"         "dest"          
## [17] "air_time"       "distance"
\end{verbatim}

Lastly, the helper functions \texttt{starts\_with}, \texttt{ends\_with},
and \texttt{contains} can be used to choose column names that match
those conditions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_begin_a <-}\StringTok{ }\KeywordTok{select}\NormalTok{(flights, }\KeywordTok{starts_with}\NormalTok{(}\StringTok{"a"}\NormalTok{))}
\NormalTok{flights_begin_a}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 336,776 x 3
##    arr_time arr_delay air_time
##       <int>     <dbl>    <dbl>
## 1       830        11      227
## 2       850        20      227
## 3       923        33      160
## 4      1004       -18      183
## 5       812       -25      116
## 6       740        12      150
## 7       913        19      158
## 8       709       -14       53
## 9       838        -8      140
## 10      753         8      138
## # ... with 336,766 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_delays <-}\StringTok{ }\KeywordTok{select}\NormalTok{(flights, }\KeywordTok{ends_with}\NormalTok{(}\StringTok{"delay"}\NormalTok{))}
\NormalTok{flights_delays}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 336,776 x 2
##    dep_delay arr_delay
##        <dbl>     <dbl>
## 1          2        11
## 2          4        20
## 3          2        33
## 4         -1       -18
## 5         -6       -25
## 6         -4        12
## 7         -5        19
## 8         -3       -14
## 9         -3        -8
## 10        -2         8
## # ... with 336,766 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_time <-}\StringTok{ }\KeywordTok{select}\NormalTok{(flights, }\KeywordTok{contains}\NormalTok{(}\StringTok{"time"}\NormalTok{))}
\NormalTok{flights_time}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 336,776 x 6
##    dep_time sched_dep_time arr_time sched_arr_time air_time
##       <int>          <int>    <int>          <int>    <dbl>
## 1       517            515      830            819      227
## 2       533            529      850            830      227
## 3       542            540      923            850      160
## 4       544            545     1004           1022      183
## 5       554            600      812            837      116
## 6       554            558      740            728      150
## 7       555            600      913            854      158
## 8       557            600      709            723       53
## 9       557            600      838            846      140
## 10      558            600      753            745      138
## # ... with 336,766 more rows, and 1 more variables: time_hour <time>
\end{verbatim}

Another useful function is \texttt{rename}, which as you may suspect
renames one column to another name. Suppose we wanted \texttt{dep\_time}
and \texttt{arr\_time} to be \texttt{departure\_time} and
\texttt{arrival\_time} instead in the \texttt{flights\_time} data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_time <-}\StringTok{ }\KeywordTok{rename}\NormalTok{(flights_time,}
                       \DataTypeTok{departure_time =} \NormalTok{dep_time,}
                       \DataTypeTok{arrival_time =} \NormalTok{arr_time)}
\KeywordTok{names}\NormalTok{(flights_time)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "departure_time" "sched_dep_time" "arrival_time"   "sched_arr_time"
## [5] "air_time"       "time_hour"
\end{verbatim}

It's easy to forget if the new name comes before or after the equals
sign. I usually remember this as ``New Before, Old After'' or NBOA.

You'll receive an error if you try to do it the other way:

\begin{verbatim}
Error: Unknown variables: departure_time, arrival_time.
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC5.1)} How many different ways are there to select all three
of \texttt{dest}, \texttt{air\_time}, and \texttt{distance} variables
from \texttt{flights}? Give the code showing how to do all of them you
can think of.

\textbf{(LC5.2)} How could one use \texttt{starts\_with},
\texttt{ends\_with}, and \texttt{contains} to select columns from a
dataset with 100 or so columns? Think up a dataset that might have that
many columns and discuss how each of these functions could be used to
make smaller data sets.

\textbf{(LC5.3)} Why might we want to use the \texttt{select} function
on a data frame?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{\texorpdfstring{Filter observations using
\texttt{filter}}{Filter observations using filter}}\label{filter-observations-using-filter}

\begin{figure}

{\centering \includegraphics[width=17.03in]{images/filter} 

}

\caption[Filter diagram from Data Wrangling with dplyr and tidyr cheatsheet]{Filter diagram from Data Wrangling with dplyr and tidyr cheatsheet}\label{fig:filter}
\end{figure}

All of the FMVs follow the same syntax with the first argument to the
function/verb being the name of the data frame and then the other
arguments specifying which criteria you'd like the verb to work with.

The \texttt{filter} function here works much like the ``Filter'' option
in Microsoft Excel. It allows you to specify criteria about values of
variable in your data set and then chooses only those rows that match
that criteria. We begin by focusing only on flights from New York City
to Portland, Oregon. The \texttt{dest} code (or airport code) for
Portland, Oregon is \texttt{"PDX"}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{portland_flights <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights, dest ==}\StringTok{ "PDX"}\NormalTok{)}
\NormalTok{portland_flights}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,354 x 18
##    month   day dep_time sched_dep_time dep_delay arr_time
##    <int> <int>    <int>          <int>     <dbl>    <int>
## 1      1     1     1739           1740        -1     2051
## 2      1     1     1805           1757         8     2117
## 3      1     1     2052           2029        23     2349
## 4      1     2      804            805        -1     1039
## 5      1     2     1552           1550         2     1853
## 6      1     2     1727           1720         7     2042
## 7      1     2     1738           1740        -2     2028
## 8      1     2     2024           2029        -5     2314
## 9      1     3     1755           1745        10     2110
## 10     1     3     1814           1727        47     2108
## # ... with 1,344 more rows, and 12 more variables:
## #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,
## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,
## #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <time>
\end{verbatim}

Note the second equals sign here. You are almost guaranteed to make the
mistake at least one of only including one equals sign. Let's see what
happens when we make this error:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{portland_flights <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights, }\DataTypeTok{dest =} \StringTok{"PDX"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error: filter() takes unnamed arguments. Do you need `==`?
\end{verbatim}

We see that there were 1354 flights from New York City to Portland in
2013. Let's combine this with what we saw in Subsection \ref{select} to
ensure that Portland flights were selected:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reordered_flights <-}\StringTok{ }\KeywordTok{select}\NormalTok{(flights, dest, }\KeywordTok{everything}\NormalTok{())}
\NormalTok{pdx_flights <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(reordered_flights, dest ==}\StringTok{ "PDX"}\NormalTok{)}
\NormalTok{pdx_flights}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,354 x 18
##     dest month   day dep_time sched_dep_time dep_delay arr_time
##    <chr> <int> <int>    <int>          <int>     <dbl>    <int>
## 1    PDX     1     1     1739           1740        -1     2051
## 2    PDX     1     1     1805           1757         8     2117
## 3    PDX     1     1     2052           2029        23     2349
## 4    PDX     1     2      804            805        -1     1039
## 5    PDX     1     2     1552           1550         2     1853
## 6    PDX     1     2     1727           1720         7     2042
## 7    PDX     1     2     1738           1740        -2     2028
## 8    PDX     1     2     2024           2029        -5     2314
## 9    PDX     1     3     1755           1745        10     2110
## 10   PDX     1     3     1814           1727        47     2108
## # ... with 1,344 more rows, and 11 more variables:
## #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,
## #   tailnum <chr>, origin <chr>, air_time <dbl>, distance <dbl>,
## #   hour <dbl>, minute <dbl>, time_hour <time>
\end{verbatim}

We see that Portland flights were selected here. You could also run
\texttt{View(pdx\_flights)} to glance at the data in spreadsheet form.

You can combine multiple criteria together using operators that make
comparisons:

\begin{itemize}
\tightlist
\item
  \texttt{\textbar{}} corresponds to ``or''
\item
  \texttt{\&} corresponds to ``and''
\end{itemize}

We can often skip the use of \texttt{\&} and just separate our
conditions with a comma. You'll see this in the example below.

In addition, you can use other mathematical checks (similar to
\texttt{==}):

\begin{itemize}
\tightlist
\item
  \texttt{\textgreater{}} corresponds to ``greater than''
\item
  \texttt{\textless{}} corresponds to ``less than''
\item
  \texttt{\textgreater{}=} corresponds to ``greater than or equal to''
\item
  \texttt{\textless{}=} corresponds to ``less than or equal to''
\item
  \texttt{!=} corresponds to ``not equal to''
\end{itemize}

To see many of these in action, let's select all flights that left JFK
airport heading to Burlington, Vermont (\texttt{"BTV"}) or Seattle,
Washington (\texttt{"SEA"}) in the months of October, November, or
December:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{btv_sea_flights_fall <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights,}
                               \NormalTok{origin ==}\StringTok{ "JFK"}\NormalTok{, }
                               \NormalTok{(dest ==}\StringTok{ "BTV"}\NormalTok{) |}\StringTok{ }\NormalTok{(dest ==}\StringTok{ "SEA"}\NormalTok{),}
                               \NormalTok{month >=}\StringTok{ }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Another example uses the \texttt{!} to choose pick rows that
\textbf{DON'T} match a condition. Here we are referring to excluding the
Northern Hemisphere summer months of June, July, and August.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{not_summer_flights <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights,}
                             \NormalTok{!}\KeywordTok{between}\NormalTok{(month, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\NormalTok{not_summer_flights}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 249,781 x 18
##    month   day dep_time sched_dep_time dep_delay arr_time
##    <int> <int>    <int>          <int>     <dbl>    <int>
## 1      1     1      517            515         2      830
## 2      1     1      533            529         4      850
## 3      1     1      542            540         2      923
## 4      1     1      544            545        -1     1004
## 5      1     1      554            600        -6      812
## 6      1     1      554            558        -4      740
## 7      1     1      555            600        -5      913
## 8      1     1      557            600        -3      709
## 9      1     1      557            600        -3      838
## 10     1     1      558            600        -2      753
## # ... with 249,771 more rows, and 12 more variables:
## #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,
## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,
## #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <time>
\end{verbatim}

To check that we are correct here we can use the \texttt{count} function
on the \texttt{month} variable in our \texttt{not\_summer\_flights} data
frame to ensure June, July, and August are not selected:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(not_summer_flights, month)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 9 x 2
##   month     n
##   <int> <int>
## 1     1 27004
## 2     2 24951
## 3     3 28834
## 4     4 28330
## 5     5 28796
## 6     9 27574
## 7    10 28889
## 8    11 27268
## 9    12 28135
\end{verbatim}

The function \texttt{between} is a shortcut. We could also have written
the following to get the same result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{not_summer2 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights, month <=}\StringTok{ }\DecValTok{5} \NormalTok{|}\StringTok{ }\NormalTok{month >=}\StringTok{ }\DecValTok{9}\NormalTok{)}
\KeywordTok{count}\NormalTok{(not_summer2, month)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 9 x 2
##   month     n
##   <int> <int>
## 1     1 27004
## 2     2 24951
## 3     3 28834
## 4     4 28330
## 5     5 28796
## 6     9 27574
## 7    10 28889
## 8    11 27268
## 9    12 28135
\end{verbatim}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC5.4)} What's another way using \texttt{!} we could filter
only the rows that are not summer months (June, July, or August) in the
\texttt{flights} data frame?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{\texorpdfstring{Summarize variables using
\texttt{summarize}}{Summarize variables using summarize}}\label{summarize-variables-using-summarize}

\begin{figure}

{\centering \includegraphics[width=16.08in]{images/summarize1} 

}

\caption[Summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet]{Summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet}\label{fig:sum1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=9.53in]{images/summary} 

}

\caption[Another summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet]{Another summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet}\label{fig:sum2}
\end{figure}

We saw in Subsection \ref{contsum} a way to calculate the standard
deviation and mean of the temperature variable \texttt{temp} in the
\texttt{weather} data frame of \texttt{nycflights}. We can do so in one
step using the \texttt{summarize} function in \texttt{dplyr}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarize}\NormalTok{(weather,}
          \DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(temp),}
          \DataTypeTok{std_dev =} \KeywordTok{sd}\NormalTok{(temp))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##    mean std_dev
##   <dbl>   <dbl>
## 1    NA      NA
\end{verbatim}

What happened here? The mean and the standard deviation temperatures are
missing? Remember that by default the \texttt{mean} and \texttt{sd}
functions do not ignore missing values. We need to specify \texttt{TRUE}
for the \texttt{na.rm} parameter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary_temp <-}\StringTok{ }\KeywordTok{summarize}\NormalTok{(weather,}
          \DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(temp, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
          \DataTypeTok{std_dev =} \KeywordTok{sd}\NormalTok{(temp, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
          \NormalTok{)}
\NormalTok{summary_temp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##       mean  std_dev
##      <dbl>    <dbl>
## 1 55.20351 17.78212
\end{verbatim}

We've created a small data frame here called \texttt{summary\_temp} that
includes both the \texttt{mean} and the \texttt{std\_dev} of the
\texttt{temp} variable in \texttt{weather}. If we'd like to access
either of these values directly we can use the \texttt{\$} to specify a
column in a data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary_temp$mean}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 55.20351
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary_temp$std_dev}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 17.78212
\end{verbatim}

It's often more useful to summarize a variable based on the groupings of
another variable. Let's say we were interested in the mean and standard
deviation of temperatures for each month. We believe that you will be
amazed at just how simple this is:

\begin{figure}

{\centering \includegraphics[width=13.11in]{images/group_summary} 

}

\caption[Group by and summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet]{Group by and summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet}\label{fig:groupsummarize}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped_weather <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(weather, month)}
\NormalTok{summary_tempXmonth <-}\StringTok{ }\KeywordTok{summarize}\NormalTok{(grouped_weather,}
          \DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(temp, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
          \DataTypeTok{std_dev =} \KeywordTok{sd}\NormalTok{(temp, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
          \NormalTok{)}
\NormalTok{summary_tempXmonth}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 12 x 3
##    month     mean   std_dev
##    <dbl>    <dbl>     <dbl>
## 1      1 35.64127 10.185459
## 2      2 34.15454  6.940228
## 3      3 39.81404  6.224948
## 4      4 51.67094  8.785250
## 5      5 61.59185  9.608687
## 6      6 72.14500  7.603356
## 7      7 80.00967  7.147631
## 8      8 74.40495  5.171365
## 9      9 67.42582  8.475824
## 10    10 60.03305  8.829652
## 11    11 45.10893 10.502249
## 12    12 38.36811  9.940822
\end{verbatim}

By simply grouping the \texttt{weather} data set by \texttt{month} first
and then passing this new data frame into \texttt{summarize} we get a
resulting data frame that shows the mean and standard deviation
temperature for each month in New York City.

Another useful function is the \texttt{n} function which gives a count
of how many entries appeared in the groupings. Suppose we'd like to get
a sense for how many flights departed each of the three airports in New
York City:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped_flights <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(flights, origin)}
\NormalTok{by_origin <-}\StringTok{ }\KeywordTok{summarize}\NormalTok{(grouped_flights,}
                       \DataTypeTok{count =} \KeywordTok{n}\NormalTok{())}
\NormalTok{by_origin}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   origin  count
##    <chr>  <int>
## 1    EWR 120835
## 2    JFK 111279
## 3    LGA 104662
\end{verbatim}

We see that Newark (\texttt{"EWR"}) had the most flights departing in
2013 followed by \texttt{"JFK"} and lastly by LaGuardia
(\texttt{"LGA"}).

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC5.5)} What does the standard deviation column in the
\texttt{summary\_tempXmonth} data frame tell us about temperatures in
New York City throughout the year?

\textbf{(LC5.6)} What code would be required to get the mean and
standard deviation temperature for each day in 2013 for NYC?

\textbf{(LC5.7)} How could we identify how many flights left each of the
three airports in each of the months of 2013?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{\texorpdfstring{Create new variables/change old variables
using
\texttt{mutate}}{Create new variables/change old variables using mutate}}\label{create-new-variableschange-old-variables-using-mutate}

\begin{figure}

{\centering \includegraphics[width=17in]{images/mutate} 

}

\caption[Mutate diagram from Data Wrangling with dplyr and tidyr cheatsheet]{Mutate diagram from Data Wrangling with dplyr and tidyr cheatsheet}\label{fig:select}
\end{figure}

When looking at the \texttt{flights} data set, there are some clear
additional variables that could be calculated based on the values of
variables already in the data set. Passengers are often frustrated when
their flights departs late, but change their mood a bit if pilots can
make up some time during the flight to get them to their destination
close to when they expected to land. This is commonly referred to as
gain and we will create this variable using the \texttt{mutate}
function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_plus <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(flights,}
         \DataTypeTok{gain =} \NormalTok{arr_delay -}\StringTok{ }\NormalTok{dep_delay)}
\end{Highlighting}
\end{Shaded}

We can now look at summary measures of this \texttt{gain} variable and
even plot it in the form of a histogram:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gain_summary <-}\StringTok{ }\KeywordTok{summarize}\NormalTok{(flights_plus,}
          \DataTypeTok{min =} \KeywordTok{min}\NormalTok{(gain, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
          \DataTypeTok{q1 =} \KeywordTok{quantile}\NormalTok{(gain, }\FloatTok{0.25}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
          \DataTypeTok{median =} \KeywordTok{quantile}\NormalTok{(gain, }\FloatTok{0.5}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
          \DataTypeTok{q3 =} \KeywordTok{quantile}\NormalTok{(gain, }\FloatTok{0.75}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
          \DataTypeTok{max =} \KeywordTok{max}\NormalTok{(gain, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
          \DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(gain, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
          \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(gain, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
          \DataTypeTok{missing =} \KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(gain))}
\NormalTok{)}
\NormalTok{gain_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 8
##     min    q1 median    q3   max      mean       sd missing
##   <dbl> <dbl>  <dbl> <dbl> <dbl>     <dbl>    <dbl>   <int>
## 1  -109   -17     -7     3   196 -5.659779 18.04365    9430
\end{verbatim}

We've recreated the \texttt{summary} function we say in Chapter
\ref{viz} here using the \texttt{summarize} function in \texttt{dplyr}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(flights_plus, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{gain)) +}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{bins =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-64-1} \end{center}

We can also create multiple columns at once and even refer to columns
that were just created in a new column. Hadley produces one such example
in Chapter 5 of ``R for Data Science'' \citep{r4ds2016}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_plus2 <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(flights,}
  \DataTypeTok{gain =} \NormalTok{arr_delay -}\StringTok{ }\NormalTok{dep_delay,}
  \DataTypeTok{hours =} \NormalTok{air_time /}\StringTok{ }\DecValTok{60}\NormalTok{,}
  \DataTypeTok{gain_per_hour =} \NormalTok{gain /}\StringTok{ }\NormalTok{hours}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC5.8)} What do positive values of the \texttt{gain} variable
in \texttt{flights\_plus} correspond to? What about negative values? And
what about a zero value?

\textbf{(LC5.9)} Could we create the \texttt{dep\_delay} and
\texttt{arr\_delay} columns by simply subtracting \texttt{dep\_time}
from \texttt{sched\_dep\_time} and similarly for arrivals? Try the code
out and explain any differences between the result and what actually
appears in \texttt{flights}.

\textbf{(LC5.10)} What can we say about the distribution of
\texttt{gain}? Describe it in a few sentences using the plot and the
\texttt{gain\_summary} data frame values.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{\texorpdfstring{Reorder the data frame using
\texttt{arrange}}{Reorder the data frame using arrange}}\label{reorder-the-data-frame-using-arrange}

If you've ever worked with data, one of the most common things you'd
like to do is sort it. Have you ever been asked to calculate a median by
hand? This requires you to put the data in order from smallest to
highest in value. The \texttt{dplyr} package has a function called
\texttt{arrange} that we will use to sort/reorder our data according to
the values of the specified variable. This is most frequently used after
we have used the \texttt{group\_by} and \texttt{summarize} functions as
we will see.

Let's suppose we were interested in determining the most frequent
destination airports from New York City in 2013:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_dest <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(flights, dest)}
\NormalTok{freq_dest <-}\StringTok{ }\KeywordTok{summarize}\NormalTok{(by_dest, }\DataTypeTok{num_flights =} \KeywordTok{n}\NormalTok{())}
\NormalTok{freq_dest}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105 x 2
##     dest num_flights
##    <chr>       <int>
## 1    ABQ         254
## 2    ACK         265
## 3    ALB         439
## 4    ANC           8
## 5    ATL       17215
## 6    AUS        2439
## 7    AVL         275
## 8    BDL         443
## 9    BGR         375
## 10   BHM         297
## # ... with 95 more rows
\end{verbatim}

You'll see that by default the values of \texttt{dest} are displayed in
alphabetical order here. Remember to use \texttt{View()} in the R
Console to look at all the values of \texttt{freq\_dest} in spreadsheet
format. We are interested in finding those airports that appear most:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(freq_dest, num_flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105 x 2
##     dest num_flights
##    <chr>       <int>
## 1    LEX           1
## 2    LGA           1
## 3    ANC           8
## 4    SBN          10
## 5    HDN          15
## 6    MTJ          15
## 7    EYW          17
## 8    PSP          19
## 9    JAC          25
## 10   BZN          36
## # ... with 95 more rows
\end{verbatim}

This is actually giving us the opposite of what we are looking for. It
tells us the least frequent destination airports. To switch the ordering
to be descending instead of ascending we use the \texttt{desc} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(freq_dest, }\KeywordTok{desc}\NormalTok{(num_flights))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105 x 2
##     dest num_flights
##    <chr>       <int>
## 1    ORD       17283
## 2    ATL       17215
## 3    LAX       16174
## 4    BOS       15508
## 5    MCO       14082
## 6    CLT       14064
## 7    SFO       13331
## 8    FLL       12055
## 9    MIA       11728
## 10   DCA        9705
## # ... with 95 more rows
\end{verbatim}

We can also use the \texttt{top\_n} function which automatically tells
us the most frequent \texttt{num\_flights}. We specify the top 10
airports here:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{top_n}\NormalTok{(freq_dest, }\DataTypeTok{n =} \DecValTok{10}\NormalTok{, }\DataTypeTok{wt =} \NormalTok{num_flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 2
##     dest num_flights
##    <chr>       <int>
## 1    ATL       17215
## 2    BOS       15508
## 3    CLT       14064
## 4    DCA        9705
## 5    FLL       12055
## 6    LAX       16174
## 7    MCO       14082
## 8    MIA       11728
## 9    ORD       17283
## 10   SFO       13331
\end{verbatim}

We'll still need to arrange this by \texttt{num\_flights} though:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(}\KeywordTok{top_n}\NormalTok{(freq_dest, }\DataTypeTok{n =} \DecValTok{10}\NormalTok{, }\DataTypeTok{wt =} \NormalTok{num_flights), }\KeywordTok{desc}\NormalTok{(num_flights))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 2
##     dest num_flights
##    <chr>       <int>
## 1    ORD       17283
## 2    ATL       17215
## 3    LAX       16174
## 4    BOS       15508
## 5    MCO       14082
## 6    CLT       14064
## 7    SFO       13331
## 8    FLL       12055
## 9    MIA       11728
## 10   DCA        9705
\end{verbatim}

\textbf{Note:} Remember that I didn't pull the \texttt{n} and
\texttt{wt} arguments out of thin air. They can be found by using the
\texttt{?} function on \texttt{top\_n}.

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC5.11)} Create a new data frame that shows the top 5 airports
with the largest arrival delays from NYC in 2013.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{\texorpdfstring{The pipe
\texttt{\%\textgreater{}\%}}{The pipe \%\textgreater{}\%}}\label{the-pipe}

Just as the \texttt{+} sign was used to add layers to a plot created
using \texttt{ggplot} we will use the pipe operator
(\texttt{\%\textgreater{}\%}) to chain together \texttt{dplyr}
functions. We'll see that we can even chain together \texttt{dplyr}
functions and plotting code. (Both \texttt{ggplot2} and \texttt{dplyr}
were created by Hadley after all.)

You may have been a little confused by the last chunk we created above:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(}\KeywordTok{top_n}\NormalTok{(freq_dest, }\DataTypeTok{n =} \DecValTok{10}\NormalTok{, }\DataTypeTok{wt =} \NormalTok{num_flights), }\KeywordTok{desc}\NormalTok{(num_flights))}
\end{Highlighting}
\end{Shaded}

If we don't create temporary variables like we did before with
\texttt{by\_dest}, \texttt{grouped\_flights}, etc., we start to get into
the issue of trying to match parentheses. We could separate this code a
bit to help with this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(}
  \KeywordTok{top_n}\NormalTok{(freq_dest, }
        \DataTypeTok{n =} \DecValTok{10}\NormalTok{,}
        \DataTypeTok{wt =} \NormalTok{num_flights), }
  \KeywordTok{desc}\NormalTok{(num_flights))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 2
##     dest num_flights
##    <chr>       <int>
## 1    ORD       17283
## 2    ATL       17215
## 3    LAX       16174
## 4    BOS       15508
## 5    MCO       14082
## 6    CLT       14064
## 7    SFO       13331
## 8    FLL       12055
## 9    MIA       11728
## 10   DCA        9705
\end{verbatim}

Even this make it difficult to understand what is exactly happening
though. \texttt{desc(num\_flights)} is an argument to \texttt{arrange}.
The best way to fix this problem is the use of the chaining operator
called the pipe (\texttt{\%\textgreater{}\%}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq_dest %>%}
\StringTok{  }\KeywordTok{top_n}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10}\NormalTok{, }\DataTypeTok{wt =} \NormalTok{num_flights) %>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(num_flights))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 2
##     dest num_flights
##    <chr>       <int>
## 1    ORD       17283
## 2    ATL       17215
## 3    LAX       16174
## 4    BOS       15508
## 5    MCO       14082
## 6    CLT       14064
## 7    SFO       13331
## 8    FLL       12055
## 9    MIA       11728
## 10   DCA        9705
\end{verbatim}

Recall from Chapter @\ref(viz) that we read the pipe operator as ``and
then''. So here we take the \texttt{freq\_dest} data frame \textbf{AND
THEN} we determine the top 10 values of \texttt{num\_flights}
\textbf{AND THEN} we arrange these top 10 flights according to a
descending numbers of flights (from highest to lowest).

We can go one stop further and tie together the \texttt{group\_by} and
\texttt{summarize} functions we used to find the most frequent flights:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ten_freq_dests <-}\StringTok{ }\NormalTok{flights %>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(dest) %>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{num_flights =} \KeywordTok{n}\NormalTok{()) %>%}
\StringTok{  }\KeywordTok{top_n}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(num_flights))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Selecting by num_flights
\end{verbatim}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC5.12)} Recreate each of the chunks of code above Subsection
\ref{arrange} in this chapter using the \texttt{\%\textgreater{}\%}
operator. Note that sometimes you can combine multiple subsequent chunks
of code together. Do so whenever possible.

\textbf{(LC5.13)} What benefits can you see to using the pipe instead of
the other way of doing things as you saw throughout this chapter? Give
specific examples.

\textbf{(LC5.14)} Write out exactly how the \texttt{ten\_freq\_dests}
data set was created using the ``and then'' verbiage.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The piping syntax will be our major focus throughout the rest of this
book and you'll find that you'll quickly be addicted to the chaining
with some practice. If you'd like to see more examples on using
\texttt{dplyr}, the FMV (in addition to some other \texttt{dplyr}
verbs), and \texttt{\%\textgreater{}\%} with the \texttt{nycflights13}
data set, you can check out Chapter 5 of Hadley and Garrett's book
\citep{r4ds2016}.

\section{Joining/merging data frames}\label{joiningmerging-data-frames}

Something you may have thought to yourself as you looked at the most
freqent destinations of flights from NYC in 2013 is

\begin{itemize}
\tightlist
\item
  ``What cities are these airports in?''
\item
  ``Is \texttt{"ORD"} Orlando?''
\item
  ``Where is \texttt{"FLL"}?
\end{itemize}

The \texttt{nycflights13} data source contains multiple data frames.
Instead of having to manually look up different values of airport names
corresponding to airport codes like \texttt{ORD}, we can have R
automatically do this ``looking up'' for us.

To do so, we'll need to tell R how to match one data frame to another
data frame. Let's first check out the \texttt{airports} data frame
inside of R:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{View}\NormalTok{(airports)}
\end{Highlighting}
\end{Shaded}

The first column \texttt{faa} corresponds to the airport codes that we
saw in \texttt{dest} in our \texttt{flights} and subsequent
\texttt{ten\_freq\_dests} data sets. Hadley and Garrett \citep{r4ds2016}
created the following diagram to help us understand how the different
data sets are linked:

\begin{figure}

{\centering \includegraphics[width=11in]{images/relational-nycflights} 

}

\caption[Data relationships in nycflights13 from R for Data Science]{Data relationships in nycflights13 from R for Data Science}\label{fig:reldiagram}
\end{figure}

We see from \texttt{View(airports)} that \texttt{airports} contains a
lot of other information about 1396. We are only really interested here
in the \texttt{faa} and \texttt{name} columns. Let's use the
\texttt{select} function to only use those variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airports_small <-}\StringTok{ }\NormalTok{airports %>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(faa, name)}
\end{Highlighting}
\end{Shaded}

So if we identify the names of the airports we can use the
\texttt{inner\_join} function. Note that we will also rename the
subsequent column \texttt{name} as \texttt{airport\_name}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{named_freq_dests <-}\StringTok{ }\NormalTok{ten_freq_dests %>%}
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(airports_small, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"dest"} \NormalTok{=}\StringTok{ "faa"}\NormalTok{)) %>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{airport_name =} \NormalTok{name)}
\NormalTok{named_freq_dests}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 3
##     dest num_flights                       airport_name
##    <chr>       <int>                              <chr>
## 1    ORD       17283                 Chicago Ohare Intl
## 2    ATL       17215    Hartsfield Jackson Atlanta Intl
## 3    LAX       16174                   Los Angeles Intl
## 4    BOS       15508 General Edward Lawrence Logan Intl
## 5    MCO       14082                       Orlando Intl
## 6    CLT       14064             Charlotte Douglas Intl
## 7    SFO       13331                 San Francisco Intl
## 8    FLL       12055     Fort Lauderdale Hollywood Intl
## 9    MIA       11728                         Miami Intl
## 10   DCA        9705      Ronald Reagan Washington Natl
\end{verbatim}

In case you didn't know, \texttt{"ORD"} is the airport code of Chicago
O'Hare airport and \texttt{"FLL"} is the main airport in Fort
Lauderdale, Florida.

A visual representation of the \texttt{inner\_join} is given
\citep{r4ds2016}:

\begin{figure}

{\centering \includegraphics[width=9.39in]{images/join-inner} 

}

\caption[Diagram of inner join from R for Data Science]{Diagram of inner join from R for Data Science}\label{fig:ijdiagram}
\end{figure}

There are more complex joins available, but the \texttt{inner\_join}
will solve nearly all (if not all) of the problems you'll face in our
experience.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC5.15)} What happens when you try to \texttt{inner\_join} the
\texttt{ten\_freq\_dests} data frame with \texttt{airports} instead of
\texttt{airports\_small}? How might one use this result to answer
further questions about the top 10 destinations?

\textbf{(LC5.16)} What surprises you about the top 10 destinations from
NYC in 2013?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{What's to come?}\label{whats-to-come-2}

This concludes the \textbf{Data Exploration} unit of this book. You
should be pretty proficient in both plotting variables (or multiple
variables together) in various data sets and manipulating data as we've
done in this chapter. You are encouraged to step back through the code
in earlier chapters and make changes as you see fit based on your
updated knowledge.

In Chapter \ref{collect}, we'll begin to build the pieces needed to
understand how this unit of \textbf{Data Exploration} can tie into
statistical inference in the \textbf{Inference} part of the book.
Remember that the focus throughout is on data visualization and we'll
see that next when we discuss sampling.

\chapter{Collecting Data}\label{collect}

More traditional in feel

\begin{itemize}
\tightlist
\item
  (random) Sampling: representativeness/generalizability/bias

  \begin{itemize}
  \tightlist
  \item
    Image of sampling: population, soup idea
  \item
    Representativeness/bias: OkCupid to make statements. Population
  \end{itemize}
\item
  Observational study vs randomized experiment (fit with correlation)

  \begin{itemize}
  \tightlist
  \item
    OkCupid is an observational study
  \end{itemize}
\item
  Correlation is not causation

  \begin{itemize}
  \tightlist
  \item
    Confounding variables
  \item
    Shoes on vs waking up with headache
  \item
    Think up data example to include in earlier chapters
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC6.1)}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{What's to come?}\label{whats-to-come-3}

\part{Inference}\label{part-inference}


\chapter{Inference Basics}\label{infer-basics}

\begin{itemize}
\tightlist
\item
  Idea of histogram of sample being a visual approximation to population
  distribution.

  \begin{itemize}
  \tightlist
  \item
    Shiny
  \item
    Take normal distribution using mean/SD of okcupid heights and plot
    count (not density) curve on top of histogram.
  \end{itemize}
\item
  Sampling distribution:

  \begin{itemize}
  \tightlist
  \item
    Permutation test:

    \begin{itemize}
    \item
      Flight delays for American vs United
    \item
      Do women smoke more than men in OkCupid?
    \item
      Think up more dichotomies than male vs female.
    \item
    \end{itemize}
  \end{itemize}
\item
  Standard errors and sampling distribution via randomization

  \begin{itemize}
  \tightlist
  \item
    Developing traditional inference from randomization
  \item
    two-sample permutation test -\textgreater{} null distribution\\
  \item
    Showing what happens when assumptions/conditions aren't met
  \item
    Then show normal/t-test and show how it fits on top of sampling
    distribution
  \item
    Shiny Google Forms
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC7.1)}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{What's to come?}\label{whats-to-come-4}

\chapter{Hypothesis Testing}\label{hypo}

\begin{itemize}
\tightlist
\item
  Hypothesis testing

  \begin{itemize}
  \tightlist
  \item
    The theory of hypothesis. Criminal justice. Question: what do you do
    with a problem like alpha?
  \item
    Example of non-rejection of H0: heights of men and women, flights
    from NYC to Boston vs flights from NYC to SF.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC8.1)}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{What's to come?}\label{whats-to-come-5}

\chapter{Confidence Intervals}\label{ci}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC9.1)}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{What's to come?}\label{whats-to-come-6}

\chapter{Simple and Multiple Regression}\label{regress}

\begin{itemize}
\tightlist
\item
  Simple linear regression
\item
  Multiple regression

  \begin{itemize}
  \tightlist
  \item
    Regression/correlation/multiple regression/confounding
  \item
    Categorical predictor and baseline
  \item
    Implement \texttt{tidy}, \texttt{broom::augment}, and
    \texttt{glance} in \texttt{broom} package to get results
  \item
    Model selection is a can of worms
  \item
    Cross-validation

    \begin{itemize}
    \tightlist
    \item
      Take half of dataset for fit, use to predict other half.

      \begin{itemize}
      \tightlist
      \item
        Show random sampling of half matters
      \item
        Is only once enough?
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{learncheck}
\textbf{\emph{Learning check}}
\end{learncheck}

\textbf{(LC10.1)}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{What's to come?}\label{whats-to-come-7}

\part{Conclusion}\label{part-conclusion}


\chapter{Concluding Remarks}\label{conclusion}

\part{Appendix}\label{part-appendix}


\chapter{Appendix A: Intermediate R}\label{appendix1}

\section{Sorted barplots}\label{sorted-barplots}

Building upon the example in Section \ref{barplots}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nycflights13)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\NormalTok{flights_table <-}\StringTok{ }\KeywordTok{table}\NormalTok{(flights$carrier)}
\NormalTok{flights_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##    9E    AA    AS    B6    DL    EV    F9    FL    HA    MQ    OO    UA 
## 18460 32729   714 54635 48110 54173   685  3260   342 26397    32 58665 
##    US    VX    WN    YV 
## 20536  5162 12275   601
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#library(dplyr)}
\CommentTok{#carrier_counts <- flights %>% count(carrier)}
\CommentTok{#carrier_counts}
\end{Highlighting}
\end{Shaded}

We can sort this table from highest to lowest counts by using the
\texttt{sort} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sorted_flights <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(flights_table, }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{names}\NormalTok{(sorted_flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "UA" "B6" "EV" "DL" "AA" "MQ" "US" "9E" "WN" "VX" "FL" "AS" "F9"
## [14] "YV" "HA" "OO"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#carrier_counts <- carrier_counts %>%}
\CommentTok{#  arrange(desc(n))}
\end{Highlighting}
\end{Shaded}

It is often preferred for barplots to be ordered corresponding to the
heights of the bars. This allows the reader to more easily compare the
ordering of different airlines in terms of departed flights
\citep{robbins2013}. We can also much more easily answer questions like
``How many airlines have more departing flights than Southwest
Airlines?''.

We can use the sorted table giving the number of flights defined as
\texttt{sorted\_flights} to \textbf{reorder} the \texttt{carrier}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{flights, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carrier)) +}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() +}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{names}\NormalTok{(sorted_flights))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{ismay_kim_files/figure-latex/unnamed-chunk-80-1} 

}

\caption[Number of flights departing NYC in 2013 by airline - Descending numbers]{Number of flights departing NYC in 2013 by airline - Descending numbers}\label{fig:unnamed-chunk-80}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#ggplot(data = carrier_counts, mapping = aes(x = carrier, y = n)) +}
\CommentTok{#  geom_bar(stat = "identity") + }
\CommentTok{#  scale_x_discrete(limits = carrier)}
\end{Highlighting}
\end{Shaded}

The last addition here specifies the values of the horizontal \texttt{x}
axis on a discrete scale to correspond to those given by the entries of
\texttt{sorted\_flights}.

** ** What are three specific questions that can be more easily answered
by looking at Figure 4.6 instead of Figure 4.5?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{itemize}
\tightlist
\item
  Changing the labels of a plot (x-axis, y-axis)
\item
  Changing the theme for ggplots (\texttt{ggthemes} package too)
\item
  Adding \texttt{code\_folding} and \texttt{code\_download} to YAML
\item
  \texttt{kable} function from \texttt{knitr}
\item
  Reading in data from files in different formats
\item
  Reshaping the data with \texttt{tidyr}
\end{itemize}

\chapter{Appendix B: Statistical Basics}\label{appendix2}

\section{Basic statistical terms}\label{basic-statistical-terms}

\begin{itemize}
\tightlist
\item
  Mean
\item
  Median
\item
  Standard deviation
\item
  Five-number summary
\item
  Distribution
\item
  Outliers
\end{itemize}

\part{Bibliography}\label{part-bibliography}


\bibliography{bib/packages.bib,bib/books.bib,bib/articles.bib}



\end{document}
